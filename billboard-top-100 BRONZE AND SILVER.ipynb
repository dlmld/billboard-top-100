{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1d8997-f415-4d7b-831e-a5dd063f4a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test only; run to clear Bronze\n",
    "# dbutils.fs.rm(\"/mnt/billboard_mount/bronze_delta/hot100_raw\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2256b028-b5a2-45bf-a0aa-2ee3f5dfd296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Test only; run to clear Bronze\n",
    "-- DROP TABLE IF EXISTS bronze.hot100_raw;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b08665-1f43-46a8-8191-0b907a133f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test only; run to clear Bronze\n",
    "# spark.sql(\"DROP TABLE IF EXISTS spark_catalog.silver.hot100_clean\")\n",
    "# dbutils.fs.rm(\"/mnt/billboard_mount/silver/hot100_clean\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef3a62a-8de8-459e-946e-8fff09ef0f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test only; run to clear Bronze\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_table}\")\n",
    "# dbutils.fs.rm(silver_delta_path, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fe86fa-ec37-455d-ad0d-a26eccf7f37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# BRONZE\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "import urllib.parse\n",
    "\n",
    "# CONFIGURE AWS MOUNT\n",
    "aws_access_key = \"access\"\n",
    "aws_secret_key = \"secret\"\n",
    "aws_bucket_name = \"billboard-hot100-project\"\n",
    "mount_name = \"billboard_mount\"\n",
    "\n",
    "# CONFIGURE BRONZE PATHS\n",
    "source_csv_path = f\"/mnt/{mount_name}/bronze/hot100_delta_*.csv\"\n",
    "bronze_table = \"bronze.hot100_raw\"\n",
    "bronze_delta_path = f\"/mnt/{mount_name}/bronze_delta/hot100_raw\"\n",
    "\n",
    "# VARIABLES FOR CONTROLLING PROCESSING\n",
    "MAX_ROWS = 2000  # Limit rows processed for testing purposes\n",
    "IS_FIRST_RUN = not DeltaTable.isDeltaTable(spark, bronze_delta_path) # On the first run, all historical data is ran \n",
    "\n",
    "# MOUNT AWS S3\n",
    "encoded_secret_key = urllib.parse.quote(aws_secret_key, \"\")\n",
    "if not any(m.mountPoint == f\"/mnt/{mount_name}\" for m in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount(f\"s3a://{aws_access_key}:{encoded_secret_key}@{aws_bucket_name}\", f\"/mnt/{mount_name}\")\n",
    "    print(f\"Mounted S3 bucket at /mnt/{mount_name}\")\n",
    "else:\n",
    "    print(f\"Mount /mnt/{mount_name} already exists\")\n",
    "\n",
    "# DEFINE SCHEMA OF INGEST\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Song\", StringType(), True),\n",
    "    StructField(\"Artist\", StringType(), True),\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"Last Week\", IntegerType(), True),\n",
    "    StructField(\"Peak Position\", IntegerType(), True),\n",
    "    StructField(\"Weeks in Charts\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# FIRST RUN: INITIALIZE BRONZE TABLE ON FIRST RUN\n",
    "if IS_FIRST_RUN:\n",
    "    print(\"First run: initializing Bronze delta table\")\n",
    "    \n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(source_csv_path) \\\n",
    "        .withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "        .limit(MAX_ROWS)\n",
    "    \n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "        .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "        .save(bronze_delta_path)\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {}\n",
    "        USING DELTA LOCATION '{}'\n",
    "        TBLPROPERTIES (\n",
    "            delta.enableChangeDataFeed = true,\n",
    "            delta.columnMapping.mode = 'name'\n",
    "        )\n",
    "    \"\"\".format(bronze_table, bronze_delta_path))\n",
    "    \n",
    "    _ = spark.sql(\"OPTIMIZE {} ZORDER BY (Date, Rank)\".format(bronze_table))\n",
    "    print(f\"Initialized Bronze table with {df.count()} rows (MAX_ROWS = {MAX_ROWS})\")\n",
    "\n",
    "# LOAD CSV FILES FROM S3\n",
    "all_csv_files = [\n",
    "    f.path for f in dbutils.fs.ls(f\"/mnt/{mount_name}/bronze/\")\n",
    "    if f.name.startswith(\"hot100_delta_\") and f.name.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# COLLECT FILES ALREADY IN BRONZE\n",
    "try:\n",
    "    processed_files = spark.sql(\"SELECT DISTINCT input_file_name() FROM {}\".format(bronze_table)) \\\n",
    "        .rdd.map(lambda x: x[0]).collect()\n",
    "except:\n",
    "    processed_files = []\n",
    "\n",
    "# LOCATE FILES NOT YET IN BRONZE\n",
    "new_files = [f for f in all_csv_files if f not in processed_files]\n",
    "\n",
    "# ENSURE THERE ARE NEW FILES IN INCREMENTAL RUNS\n",
    "if not new_files:\n",
    "    df_new = spark.createDataFrame([], schema)\n",
    "    print(\"No new files found\")\n",
    "else:\n",
    "    print(f\"Found {len(new_files)} new file(s)\")\n",
    "    df_new = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(new_files) \\\n",
    "        .withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # FILTER ONLY NEW RECORDS USING LEFT ANTI JOIN TO EXCLUSE EXISTING RECORDS\n",
    "    existing_dates = spark.table(bronze_table).select(\"Date\").distinct()\n",
    "    df_new = df_new.join(existing_dates, \"Date\", \"left_anti\")\n",
    "    \n",
    "    if df_new.count() == 0:\n",
    "        print(\"No new dates found in new files. Skipping append.\")\n",
    "        df_new = spark.createDataFrame([], schema)\n",
    "    else:\n",
    "        new_date_count = df_new.select(\"Date\").distinct().count()\n",
    "        print(f\"Found {new_date_count} new dates in new files\")\n",
    "        \n",
    "        # FOR TESTING: CAP TO MAX_ROWS TO REDUCE UNNECESSARY PROCESSING\n",
    "        spark.sql(\"REFRESH TABLE {}\".format(bronze_table))\n",
    "        current_count = spark.table(bronze_table).count()\n",
    "        print(f\"Current rows: {current_count}, MAX_ROWS: {MAX_ROWS}\")\n",
    "        \n",
    "        # CALCULATE NUMBER OF ROWS TO ADD BASED ON MAX_ROWS VALUE\n",
    "        if current_count >= MAX_ROWS:\n",
    "            df_new = spark.createDataFrame([], schema)\n",
    "            print(\"MAX_ROWS reached. Skipping append.\")\n",
    "        else:\n",
    "            to_take = MAX_ROWS - current_count\n",
    "            df_new = df_new.limit(to_take)\n",
    "            print(f\"Taking {to_take} rows to reach MAX_ROWS={MAX_ROWS}\")\n",
    "\n",
    "# WITH NEW ROWS ISOLATED, APPEND TO BRONZE\n",
    "if df_new.count() > 0:\n",
    "    df_new.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_table)\n",
    "    print(f\"Appended {df_new.count()} new rows\")\n",
    "else:\n",
    "    print(\"No new rows\")\n",
    "\n",
    "# OPTIMIZE DELTA TABLE\n",
    "_ = spark.sql(\"OPTIMIZE {} ZORDER BY (Date, Rank)\".format(bronze_table))\n",
    "print(\"Bronze ingestion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f398a0b0-1ffa-412a-b567-ff3fbcf79626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SILVER\n",
    "\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, max, row_number\n",
    "from pyspark.sql.types import StringType, IntegerType, BooleanType, FloatType, DateType, TimestampType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "# DEFINE PATHS AND TABLES\n",
    "bronze_table = \"bronze.hot100_raw\"\n",
    "silver_table = \"silver.hot100_clean\"\n",
    "silver_delta_path = \"/mnt/billboard_mount/silver/hot100_clean\"\n",
    "\n",
    "# DEFINE SILVER SCHEMA\n",
    "silver_schema = StructType([\n",
    "    StructField(\"Song\", StringType(), True),\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Artist\", StringType(), True),\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"processed_date\", TimestampType(), True),\n",
    "    StructField(\"Image_URL\", StringType(), True),\n",
    "    StructField(\"Duration\", IntegerType(), True),\n",
    "    StructField(\"Explicit\", BooleanType(), True),\n",
    "    StructField(\"Song_Release_Date\", StringType(), True),\n",
    "    StructField(\"Track_Number\", IntegerType(), True),\n",
    "    StructField(\"Danceability\", FloatType(), True),\n",
    "    StructField(\"Energy\", FloatType(), True),\n",
    "    StructField(\"Key\", IntegerType(), True),\n",
    "    StructField(\"Loudness\", FloatType(), True),\n",
    "    StructField(\"Mode\", IntegerType(), True),\n",
    "    StructField(\"Speechiness\", FloatType(), True),\n",
    "    StructField(\"Acousticness\", FloatType(), True),\n",
    "    StructField(\"Instrumentalness\", FloatType(), True),\n",
    "    StructField(\"Liveness\", FloatType(), True),\n",
    "    StructField(\"Valence\", FloatType(), True),\n",
    "    StructField(\"Tempo\", FloatType(), True),\n",
    "    StructField(\"Track_ID\", StringType(), True),\n",
    "    StructField(\"Artist_ID\", StringType(), True),\n",
    "    StructField(\"In Spotify API\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# CREATE SCHEMA FOR SILVER (FIRST RUN)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "\n",
    "# DEFINE SCHEMA FOR SILVER\n",
    "# EXCLUDING COLUMNS FROM BRONZE: PEAK POSITION, LAST WEEK, WEEKS IN CHARTS. THESE VALUES IN THE DATA SOURCE ARE BASED ON THE TOP 100, IN THIS CASE JUST INTERESTED IN TRACKS PLACEMENT IN TOP 10 TO LIMIT PROCESSING. THEY WILL BE RECALCULATED IN GOLD LAYER.\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {silver_table} (\n",
    "            Song STRING,\n",
    "            Date DATE,\n",
    "            Artist STRING,\n",
    "            Rank INT,\n",
    "            processed_date TIMESTAMP,\n",
    "            Image_URL STRING,\n",
    "            Duration INT,\n",
    "            Explicit BOOLEAN,\n",
    "            Song_Release_Date STRING,\n",
    "            Track_Number INT,\n",
    "            Danceability FLOAT,\n",
    "            Energy FLOAT,\n",
    "            `Key` INT,\n",
    "            Loudness FLOAT,\n",
    "            Mode INT,\n",
    "            Speechiness FLOAT,\n",
    "            Acousticness FLOAT,\n",
    "            Instrumentalness FLOAT,\n",
    "            Liveness FLOAT,\n",
    "            Valence FLOAT,\n",
    "            Tempo FLOAT,\n",
    "            Track_ID STRING,\n",
    "            Artist_ID STRING,\n",
    "            `In Spotify API` BOOLEAN\n",
    "        )\n",
    "        USING DELTA\n",
    "        LOCATION '{silver_delta_path}'\n",
    "        TBLPROPERTIES (\n",
    "            delta.enableChangeDataFeed = true,\n",
    "            delta.columnMapping.mode = 'name'\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(f\"Silver table {silver_table} checked/created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating silver table: {e}\")\n",
    "    raise\n",
    "\n",
    "# USE CHANGE DATA FEED (CDF) TO READ ONLY CHANGES TO THE BRONZE TABLE FOR INCREMENTAL PROCESSING\n",
    "try:\n",
    "    spark.sql(f\"REFRESH TABLE {bronze_table}\")\n",
    "    latest_version = spark.sql(f\"DESCRIBE HISTORY {bronze_table}\").select(max(\"version\")).collect()[0][0]\n",
    "    starting_version = latest_version - 1 if latest_version > 0 else 0\n",
    "\n",
    "    df_bronze_changes = spark.read.format(\"delta\") \\\n",
    "        .option(\"readChangeFeed\", \"true\") \\\n",
    "        .option(\"startingVersion\", starting_version) \\\n",
    "        .table(bronze_table)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read bronze table with CDF: {e}\")\n",
    "    raise\n",
    "\n",
    "# CONVERT TO DF OBJECT\n",
    "silver_existing_df = spark.read.format(\"delta\").table(silver_table)\n",
    "\n",
    "# FILTER FOR INSERTS WITH EXPLICIT COL SELECTION\n",
    "df_silver_input = df_bronze_changes.where(\"_change_type = 'insert'\") \\\n",
    "    .select(\"Song\", \"Date\", \"Artist\", \"Rank\") \\\n",
    "    .drop(\"_change_type\", \"_commit_version\")\n",
    "\n",
    "# FILTER ONLY TOP 10\n",
    "df_silver = df_silver_input.filter(\"Rank BETWEEN 1 AND 10\")\n",
    "\n",
    "# ADD ROW NUMBERS TO DF SILVER (NEW RECORDS ONLY) FOR BATCHING\n",
    "window = Window.orderBy(\"Date\", \"Rank\", \"Song\", \"Artist\")\n",
    "df_silver = df_silver.withColumn(\"rn\", row_number().over(window))\n",
    "\n",
    "# ADD NEW COLUMNS FOR API DATA ENRICHMENT\n",
    "df_silver = df_silver \\\n",
    "    .withColumn(\"processed_date\", current_timestamp()) \\\n",
    "    .withColumn(\"Image_URL\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"Duration\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Explicit\", lit(None).cast(BooleanType())) \\\n",
    "    .withColumn(\"Song_Release_Date\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"Track_Number\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Danceability\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Energy\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Key\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Loudness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Mode\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Speechiness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Acousticness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Instrumentalness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Liveness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Valence\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Tempo\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Track_ID\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"Artist_ID\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"In Spotify API\", lit(False))\n",
    "\n",
    "# SPOTIFY TOKEN FUNCTION\n",
    "def get_spotify_token(client_id, client_secret):\n",
    "    url = \"https://accounts.spotify.com/api/token\"\n",
    "    payload = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        response.raise_for_status()\n",
    "        token_data = response.json()\n",
    "        return token_data.get(\"access_token\"), token_data.get(\"expires_in\", 3600)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error getting Spotify token: {e}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        raise\n",
    "\n",
    "# OBTAIN INITIAL SPOTIFY TOKEN\n",
    "print(\"Getting Spotify Creds\")\n",
    "client_id = \"id\"\n",
    "client_secret = \"secret\"\n",
    "access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "token_timestamp = time.time()\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "print(f\"token acquired: {access_token}\")\n",
    "\n",
    "# DEFINE VARIABLES FOR DATA ENRICHMENT LOOP\n",
    "BATCH_SIZE = 10\n",
    "total = df_silver.count()\n",
    "\n",
    "if total > 0:\n",
    "    print(f\"Enriching {total} new top-10 rows in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "    # CACHE EXISTING SILVER DATA\n",
    "    silver_existing = spark.table(silver_table) \\\n",
    "        .filter(col(\"In Spotify API\").isNotNull()) \\\n",
    "        .select(\"Song\", \"Artist\", \"Track_ID\", \"Artist_ID\", \"Duration\", \"Explicit\",\n",
    "                \"Song_Release_Date\", \"Track_Number\", \"Danceability\", \"Energy\",\n",
    "                \"Key\", \"Loudness\", \"Mode\", \"Speechiness\", \"Acousticness\", \"Instrumentalness\",\n",
    "                \"Liveness\", \"Valence\", \"Tempo\", \"Image_URL\", \"In Spotify API\") \\\n",
    "        .collect()\n",
    "\n",
    "    silver_dict = { (r.Song, r.Artist): r.asDict() for r in silver_existing }\n",
    "\n",
    "    search_url = \"https://api.spotify.com/v1/search\"\n",
    "    all_rows = df_silver.orderBy(\"rn\").collect()\n",
    "\n",
    "    # USED IN THE LOOP FOR REMOVING INFORMATION IN PARENTHESIS IN TRACK NAMES WHICH CAN CAUSE SPOTIFY SEARCH API TO FAIL TO LOCATE A TRACK\n",
    "    pattern = r'\\((.*?)\\)'\n",
    "\n",
    "    for i in range(0, total, BATCH_SIZE):\n",
    "        # REFRESH SPOTIFY TOKEN IF APPROACHING EXPIRATION\n",
    "        if time.time() - token_timestamp > token_expiry - 10:\n",
    "            print(\"Refreshing Spotify token\")\n",
    "            access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "            headers = {\"Authorization Auth acquired\": f\"Bearer {access_token}\"}\n",
    "            token_timestamp = time.time()\n",
    "\n",
    "        rows = all_rows[i:i + BATCH_SIZE]\n",
    "        print(f\"Batch {i//BATCH_SIZE + 1}: {len(rows)} rows\")\n",
    "        \n",
    "        # SET UP LOOP VARIABLES\n",
    "        rows = [row.asDict() for row in rows]\n",
    "        enriched_rows = []\n",
    "\n",
    "        # LOOP THROUGH EACH TRACK IN THE BATCH\n",
    "        for row in rows:\n",
    "            song = row[\"Song\"]\n",
    "            artist = row[\"Artist\"]\n",
    "            key = (song, artist)\n",
    "\n",
    "            # REFRESH TOKEN IF APPROACHING EXPIRATION\n",
    "            if time.time() - token_timestamp > token_expiry - 60:\n",
    "                print(f\"Token near expiry. Refreshing before calling API for: {song} - {artist}\")\n",
    "                access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "                headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "                token_timestamp = time.time()\n",
    "                print(f\"New token acquired, expires in {token_expiry} seconds.\")\n",
    "\n",
    "            # CHECK IF TRACK ALREADY EXISTS FROM A PREVIOUS WEEKLY TOP 10 AND \"In Spotify API\" HAS A VALUE (MEANING IT HAS BEEN PROCESSED BEFORE). IF IT DOES, COPY API ENRICHMENT VALUES OVER AND UPDATE WHERE APPROPRIATE.\n",
    "            if key in silver_dict and silver_dict[key][\"In Spotify API\"]:\n",
    "                print(f\"Reusing cached data for: {song} - {artist}\")\n",
    "                cached = silver_dict[key].copy()\n",
    "                cached.update({\n",
    "                    \"Song\": row[\"Song\"], \"Date\": row[\"Date\"], \"Artist\": row[\"Artist\"],\n",
    "                    \"Rank\": row[\"Rank\"],\n",
    "                    \"processed_date\": row[\"processed_date\"], \"In Spotify API\": True\n",
    "                })\n",
    "                enriched_rows.append(cached)\n",
    "                continue\n",
    "\n",
    "            # IF TRACK IS NOT IN SILVER YET, PREPARE TO CALL APIs TO PROVIDE INFORMATION ON THE TRACK\n",
    "            print(f\"Calling Spotify search for: {song} - {artist}\")\n",
    "\n",
    "            # SONG TITLES MAY HAVE WORDS IN PARENTHESES THAT CAN CAUSE THE SPOTIFY SEARCH TO NOT IDENTIFY THE SONG. REMOVE THE WORDS IN PARENTHESES. \n",
    "            match = re.search(pattern, song)\n",
    "\n",
    "            # DEFINING NEW VAR song_query SO THAT song IS NOT MODIFIED\n",
    "            song_query = song\n",
    "            if match:\n",
    "                print(f\"removing parenteses from song: {song_query}\")\n",
    "                trimmed_song_query = re.sub(r'\\([^)]*\\)', '', song_query).rstrip()\n",
    "                print(f\"removed parentheses from song. Now searching for {trimmed_song_query}\")\n",
    "                \n",
    "                # THERE MAY BE A CASE WHERE AN ENTIRE SONGNAME IS IN PARENTHESES, IN WHICH CASE THE SONG NAME SHOULD NOT HAVE THE PARENTHESES OR CONTENT BETWEEN PARENTHESES REMOVED FOR THE SEARCH\n",
    "                if len(trimmed_song_query) > 0:\n",
    "                    song_query = trimmed_song_query\n",
    "\n",
    "            query = f\"track:{song_query} artist:{artist}\"\n",
    "            params = {\"q\": query, \"type\": \"track\", \"limit\": 1}\n",
    "\n",
    "            # INTIALIZE VALUES FOR ENRICHED ROW\n",
    "            enriched_row = {\n",
    "                \"Song\": song, \"Date\": row[\"Date\"], \"Artist\": artist, \"Rank\": row[\"Rank\"],\n",
    "                \"processed_date\": row[\"processed_date\"],\n",
    "                \"Image_URL\": None, \"Duration\": None, \"Explicit\": None, \"Song_Release_Date\": None,\n",
    "                \"Track_Number\": None, \"Danceability\": None, \"Energy\": None,\n",
    "                \"Key\": None, \"Loudness\": None, \"Mode\": None, \"Speechiness\": None,\n",
    "                \"Acousticness\": None, \"Instrumentalness\": None, \"Liveness\": None,\n",
    "                \"Valence\": None, \"Tempo\": None, \"Track_ID\": None, \"Artist_ID\": None,\n",
    "                \"In Spotify API\": False\n",
    "            }\n",
    "\n",
    "            api_success = False\n",
    "            for attempt in range(2):  # TWO TRIES ALLOTTED SO IT WILL TRY AGAIN IF 401 ERROR (MEANING A TOKEN ISSUE)\n",
    "                try:\n",
    "                    response = requests.get(search_url, headers=headers, params=params, timeout=10)\n",
    "                    \n",
    "                    if response.status_code == 401:\n",
    "                        print(f\"401 Unauthorized on attempt {attempt + 1}. Forcing token refresh...\")\n",
    "                        access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "                        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "                        token_timestamp = time.time()\n",
    "                        print(f\"Refreshed token. Retrying...\")\n",
    "                        continue  # AFTER GETTING FRESH TOKEN, TRY AGAIN\n",
    "\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "\n",
    "                    # IF THERE IS DATA FOR THE TRACK FROM THE SPOTIFY API..\n",
    "                    if data[\"tracks\"][\"items\"]:\n",
    "                        track = data[\"tracks\"][\"items\"][0]\n",
    "                        track_id = track[\"id\"]\n",
    "                        print(f\"Spotify API provided track id: {track_id}\")\n",
    "\n",
    "                        # .. CAN INCLUDE SPOTIFY API VALUES IN THE ENRICHED ROW LOOPING VARIABLE TO LATER APPEND TO THE TRACK IN SILVER..\n",
    "                        enriched_row.update({\n",
    "                            \"Track_ID\": track_id,\n",
    "                            \"Artist_ID\": track[\"artists\"][0][\"id\"] if track[\"artists\"] else None,\n",
    "                            \"Image_URL\": track[\"album\"][\"images\"][0][\"url\"] if track[\"album\"][\"images\"] else None,\n",
    "                            \"Duration\": track[\"duration_ms\"],\n",
    "                            \"Explicit\": track[\"explicit\"],\n",
    "                            \"Song_Release_Date\": track[\"album\"][\"release_date\"],\n",
    "                            \"Track_Number\": track[\"track_number\"],\n",
    "                            \"In Spotify API\": True\n",
    "                        })\n",
    "\n",
    "                        # .. ALSO CAN TRY TO CALL RECCOBEATS API FOR ADDITIONAL INFORMATION ON THE TRACK\n",
    "                        try:\n",
    "                            print(f\"Calling Reccobeats API for: {song} - {artist}\")\n",
    "                            url = f\"https://api.reccobeats.com/v1/track?ids={track_id}\"\n",
    "                            recco_response = requests.get(url, headers={'Accept': 'application/json'}, timeout=10)\n",
    "                            recco_response.raise_for_status()\n",
    "                            recco_id = recco_response.json()[\"content\"][0][\"id\"] # RECCOBEATS ID IS DIFFERENT THAN SPOTIFY ID\n",
    "                            print(\"Reccobeats API success\")\n",
    "\n",
    "                            # IF RECCOBEATS API CALL IS SUCCESSFUL, CAN INCLUDE RECCOBEATS API VALUES IN THE ENRICHED ROW LOOPING VARIABLE TO LATER APPEND TO THE TRACK IN SILVER.\n",
    "                            feat_url = f\"https://api.reccobeats.com/v1/track/{recco_id}/audio-features\"\n",
    "                            feat = requests.get(feat_url, headers={'Accept': 'application/json'}, timeout=10).json()\n",
    "                            enriched_row.update({\n",
    "                                \"Danceability\":      feat.get(\"danceability\"),\n",
    "                                \"Energy\":            feat.get(\"energy\"),\n",
    "                                \"Key\":               feat.get(\"key\"),\n",
    "                                \"Loudness\":          feat.get(\"loudness\"),\n",
    "                                \"Mode\":              feat.get(\"mode\"),\n",
    "                                \"Speechiness\":       feat.get(\"speechiness\"),\n",
    "                                \"Acousticness\":      feat.get(\"acousticness\"),\n",
    "                                \"Instrumentalness\":  feat.get(\"instrumentalness\"),\n",
    "                                \"Liveness\":          feat.get(\"liveness\"),\n",
    "                                \"Valence\":           feat.get(\"valence\"),\n",
    "                                \"Tempo\":             feat.get(\"tempo\")\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(f\"Reccobeats failed for {track_id}: {e}\")\n",
    "                            # IF RECCOBEATS FAILED, PROCEED WITHOUT ENRICHING THE ROW\n",
    "\n",
    "                    else:\n",
    "                        print(f\"No tracks found for {song} - {artist}\")\n",
    "                        enriched_row[\"In Spotify API\"] = False\n",
    "                    \n",
    "                    break\n",
    "\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"API error (attempt {attempt + 1}) for {song} - {artist}: {e}\")\n",
    "                    if response and hasattr(response, 'text'):\n",
    "                        print(f\"Response: {response.text}\")\n",
    "                    if attempt == 1:\n",
    "                        enriched_row[\"In Spotify API\"] = False\n",
    "                    time.sleep(1)\n",
    "\n",
    "            # INCLUDE ROW FOR ENRICHED TRACK DATA IN CURRENT BATCH OF ENRICHED ROWS\n",
    "            enriched_rows.append(enriched_row)\n",
    "\n",
    "            # ADD TO SILVER DICT TO ALLOW FOR REUSING APPROPRIATE TRACK DATA IF TRACK IS IN A TOP 10 LIST IN A SUBSEQUENT WEEK\n",
    "            silver_dict[key] = enriched_row\n",
    "\n",
    "            time.sleep(1)  # DELAY TO PREVENT SPOTIFY API LIMIT       \n",
    "\n",
    "        print(f\"Enriched {len(enriched_rows)} rows for batch {i//BATCH_SIZE + 1}\")\n",
    "\n",
    "        # APPEND BATCH OF 10 ROWS\n",
    "        if enriched_rows:\n",
    "            spark.createDataFrame(enriched_rows, schema=silver_schema).write \\\n",
    "                .format(\"delta\").mode(\"append\").saveAsTable(silver_table)\n",
    "            print(f\"  Appended batch {i//BATCH_SIZE + 1}\")\n",
    "        else:\n",
    "            print(f\"  No rows to append for batch {i//BATCH_SIZE + 1}\")\n",
    "\n",
    "else:\n",
    "    print(\"No new songs to append.\")\n",
    "\n",
    "# PRINT FINAL COUNT OF SILVER TABLE\n",
    "final_count = spark.table(silver_table).count()\n",
    "print(f\"Silver table final row count: {final_count}\")\n",
    "\n",
    "# OPTIMIZE AND Z ORDER\n",
    "_ = spark.sql(f\"OPTIMIZE {silver_table} ZORDER BY (Date, Rank)\")\n",
    "print(\"Silver optimized with ZORDER by (Date, Rank). Ingestion complete.\")\n",
    "\n",
    "# TABLE TO SHOW SPOTIFY SUCCESS RATE\n",
    "print(\"Spotify enrichment status:\")\n",
    "spark.table(silver_table).groupBy(\"In Spotify API\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87b5d8e4-08ea-4ec2-8dd4-3056264b6ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6211216991146406,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "billboard-top-100 BRONZE AND SILVER",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
