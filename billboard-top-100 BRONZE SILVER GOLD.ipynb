{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1d8997-f415-4d7b-831e-a5dd063f4a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test only; run to clear Bronze\n",
    "# dbutils.fs.rm(\"/mnt/billboard_mount/bronze_delta/hot100_raw\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2256b028-b5a2-45bf-a0aa-2ee3f5dfd296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Test only; run to clear Bronze\n",
    "-- DROP TABLE IF EXISTS bronze.hot100_raw;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b08665-1f43-46a8-8191-0b907a133f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test only; run to clear Bronze\n",
    "# spark.sql(\"DROP TABLE IF EXISTS spark_catalog.silver.hot100_clean\")\n",
    "# dbutils.fs.rm(\"/mnt/billboard_mount/silver/hot100_clean\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef3a62a-8de8-459e-946e-8fff09ef0f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test only; run to clear silver\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_table}\")\n",
    "# dbutils.fs.rm(silver_delta_path, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fe86fa-ec37-455d-ad0d-a26eccf7f37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# BRONZE\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "import urllib.parse\n",
    "\n",
    "# CONFIGURE AWS MOUNT\n",
    "aws_access_key = \"access\"\n",
    "aws_secret_key = \"secret\"\n",
    "aws_bucket_name = \"billboard-hot100-project\"\n",
    "mount_name = \"billboard_mount\"\n",
    "\n",
    "# CONFIGURE BRONZE PATHS\n",
    "source_csv_path = f\"/mnt/{mount_name}/bronze/hot100_delta_*.csv\"\n",
    "bronze_table = \"bronze.hot100_raw\"\n",
    "bronze_delta_path = f\"/mnt/{mount_name}/bronze_delta/hot100_raw\"\n",
    "\n",
    "# VARIABLES FOR CONTROLLING PROCESSING\n",
    "MAX_ROWS = 2000  # Limit rows processed for testing purposes\n",
    "IS_FIRST_RUN = not DeltaTable.isDeltaTable(spark, bronze_delta_path) # On the first run, all historical data is ran \n",
    "\n",
    "# MOUNT AWS S3\n",
    "encoded_secret_key = urllib.parse.quote(aws_secret_key, \"\")\n",
    "if not any(m.mountPoint == f\"/mnt/{mount_name}\" for m in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount(f\"s3a://{aws_access_key}:{encoded_secret_key}@{aws_bucket_name}\", f\"/mnt/{mount_name}\")\n",
    "    print(f\"Mounted S3 bucket at /mnt/{mount_name}\")\n",
    "else:\n",
    "    print(f\"Mount /mnt/{mount_name} already exists\")\n",
    "\n",
    "# DEFINE SCHEMA OF INGEST\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Song\", StringType(), True),\n",
    "    StructField(\"Artist\", StringType(), True),\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"Last Week\", IntegerType(), True),\n",
    "    StructField(\"Peak Position\", IntegerType(), True),\n",
    "    StructField(\"Weeks in Charts\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# FIRST RUN: INITIALIZE BRONZE TABLE ON FIRST RUN\n",
    "if IS_FIRST_RUN:\n",
    "    print(\"First run: initializing Bronze delta table\")\n",
    "    \n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(source_csv_path) \\\n",
    "        .withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "        .limit(MAX_ROWS)\n",
    "    \n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "        .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "        .save(bronze_delta_path)\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {}\n",
    "        USING DELTA LOCATION '{}'\n",
    "        TBLPROPERTIES (\n",
    "            delta.enableChangeDataFeed = true,\n",
    "            delta.columnMapping.mode = 'name'\n",
    "        )\n",
    "    \"\"\".format(bronze_table, bronze_delta_path))\n",
    "    \n",
    "    _ = spark.sql(\"OPTIMIZE {} ZORDER BY (Date, Rank)\".format(bronze_table))\n",
    "    print(f\"Initialized Bronze table with {df.count()} rows (MAX_ROWS = {MAX_ROWS})\")\n",
    "\n",
    "# LOAD CSV FILES FROM S3\n",
    "all_csv_files = [\n",
    "    f.path for f in dbutils.fs.ls(f\"/mnt/{mount_name}/bronze/\")\n",
    "    if f.name.startswith(\"hot100_delta_\") and f.name.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# COLLECT FILES ALREADY IN BRONZE\n",
    "try:\n",
    "    processed_files = spark.sql(\"SELECT DISTINCT input_file_name() FROM {}\".format(bronze_table)) \\\n",
    "        .rdd.map(lambda x: x[0]).collect()\n",
    "except:\n",
    "    processed_files = []\n",
    "\n",
    "# LOCATE FILES NOT YET IN BRONZE\n",
    "new_files = [f for f in all_csv_files if f not in processed_files]\n",
    "\n",
    "# ENSURE THERE ARE NEW FILES IN INCREMENTAL RUNS\n",
    "if not new_files:\n",
    "    df_new = spark.createDataFrame([], schema)\n",
    "    print(\"No new files found\")\n",
    "else:\n",
    "    print(f\"Found {len(new_files)} new file(s)\")\n",
    "    df_new = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(new_files) \\\n",
    "        .withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # FILTER ONLY NEW RECORDS USING LEFT ANTI JOIN TO EXCLUSE EXISTING RECORDS\n",
    "    existing_dates = spark.table(bronze_table).select(\"Date\").distinct()\n",
    "    df_new = df_new.join(existing_dates, \"Date\", \"left_anti\")\n",
    "    \n",
    "    if df_new.count() == 0:\n",
    "        print(\"No new dates found in new files. Skipping append.\")\n",
    "        df_new = spark.createDataFrame([], schema)\n",
    "    else:\n",
    "        new_date_count = df_new.select(\"Date\").distinct().count()\n",
    "        print(f\"Found {new_date_count} new dates in new files\")\n",
    "        \n",
    "        # FOR TESTING: CAP TO MAX_ROWS TO REDUCE UNNECESSARY PROCESSING\n",
    "        spark.sql(\"REFRESH TABLE {}\".format(bronze_table))\n",
    "        current_count = spark.table(bronze_table).count()\n",
    "        print(f\"Current rows: {current_count}, MAX_ROWS: {MAX_ROWS}\")\n",
    "        \n",
    "        # CALCULATE NUMBER OF ROWS TO ADD BASED ON MAX_ROWS VALUE\n",
    "        if current_count >= MAX_ROWS:\n",
    "            df_new = spark.createDataFrame([], schema)\n",
    "            print(\"MAX_ROWS reached. Skipping append.\")\n",
    "        else:\n",
    "            to_take = MAX_ROWS - current_count\n",
    "            df_new = df_new.limit(to_take)\n",
    "            print(f\"Taking {to_take} rows to reach MAX_ROWS={MAX_ROWS}\")\n",
    "\n",
    "# WITH NEW ROWS ISOLATED, APPEND TO BRONZE\n",
    "if df_new.count() > 0:\n",
    "    df_new.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_table)\n",
    "    print(f\"Appended {df_new.count()} new rows\")\n",
    "else:\n",
    "    print(\"No new rows\")\n",
    "\n",
    "# OPTIMIZE DELTA TABLE\n",
    "_ = spark.sql(\"OPTIMIZE {} ZORDER BY (Date, Rank)\".format(bronze_table))\n",
    "print(\"Bronze ingestion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f398a0b0-1ffa-412a-b567-ff3fbcf79626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SILVER\n",
    "\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, max, row_number\n",
    "from pyspark.sql.types import StringType, IntegerType, BooleanType, FloatType, DateType, TimestampType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "# DEFINE PATHS AND TABLES\n",
    "bronze_table = \"bronze.hot100_raw\"\n",
    "silver_table = \"silver.hot100_clean\"\n",
    "silver_delta_path = \"/mnt/billboard_mount/silver/hot100_clean\"\n",
    "\n",
    "# DEFINE SILVER SCHEMA\n",
    "silver_schema = StructType([\n",
    "    StructField(\"Song\", StringType(), True),\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Artist\", StringType(), True),\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"processed_date\", TimestampType(), True),\n",
    "    StructField(\"Image_URL\", StringType(), True),\n",
    "    StructField(\"Duration\", IntegerType(), True),\n",
    "    StructField(\"Explicit\", BooleanType(), True),\n",
    "    StructField(\"Song_Release_Date\", StringType(), True),\n",
    "    StructField(\"Track_Number\", IntegerType(), True),\n",
    "    StructField(\"Danceability\", FloatType(), True),\n",
    "    StructField(\"Energy\", FloatType(), True),\n",
    "    StructField(\"Key\", IntegerType(), True),\n",
    "    StructField(\"Loudness\", FloatType(), True),\n",
    "    StructField(\"Mode\", IntegerType(), True),\n",
    "    StructField(\"Speechiness\", FloatType(), True),\n",
    "    StructField(\"Acousticness\", FloatType(), True),\n",
    "    StructField(\"Instrumentalness\", FloatType(), True),\n",
    "    StructField(\"Liveness\", FloatType(), True),\n",
    "    StructField(\"Valence\", FloatType(), True),\n",
    "    StructField(\"Tempo\", FloatType(), True),\n",
    "    StructField(\"Track_ID\", StringType(), True),\n",
    "    StructField(\"Artist_ID\", StringType(), True),\n",
    "    StructField(\"In Spotify API\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# CREATE SCHEMA FOR SILVER (FIRST RUN)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "\n",
    "# DEFINE SCHEMA FOR SILVER\n",
    "# EXCLUDING COLUMNS FROM BRONZE: PEAK POSITION, LAST WEEK, WEEKS IN CHARTS. THESE VALUES IN THE DATA SOURCE ARE BASED ON THE TOP 100, IN THIS CASE JUST INTERESTED IN TRACKS PLACEMENT IN TOP 10 TO LIMIT PROCESSING. THEY WILL BE RECALCULATED IN GOLD LAYER.\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {silver_table} (\n",
    "            Song STRING,\n",
    "            Date DATE,\n",
    "            Artist STRING,\n",
    "            Rank INT,\n",
    "            processed_date TIMESTAMP,\n",
    "            Image_URL STRING,\n",
    "            Duration INT,\n",
    "            Explicit BOOLEAN,\n",
    "            Song_Release_Date STRING,\n",
    "            Track_Number INT,\n",
    "            Danceability FLOAT,\n",
    "            Energy FLOAT,\n",
    "            `Key` INT,\n",
    "            Loudness FLOAT,\n",
    "            Mode INT,\n",
    "            Speechiness FLOAT,\n",
    "            Acousticness FLOAT,\n",
    "            Instrumentalness FLOAT,\n",
    "            Liveness FLOAT,\n",
    "            Valence FLOAT,\n",
    "            Tempo FLOAT,\n",
    "            Track_ID STRING,\n",
    "            Artist_ID STRING,\n",
    "            `In Spotify API` BOOLEAN\n",
    "        )\n",
    "        USING DELTA\n",
    "        LOCATION '{silver_delta_path}'\n",
    "        TBLPROPERTIES (\n",
    "            delta.enableChangeDataFeed = true,\n",
    "            delta.columnMapping.mode = 'name'\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(f\"Silver table {silver_table} checked/created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating silver table: {e}\")\n",
    "    raise\n",
    "\n",
    "# USE CHANGE DATA FEED (CDF) TO READ ONLY CHANGES TO THE BRONZE TABLE FOR INCREMENTAL PROCESSING\n",
    "try:\n",
    "    spark.sql(f\"REFRESH TABLE {bronze_table}\")\n",
    "    latest_version = spark.sql(f\"DESCRIBE HISTORY {bronze_table}\").select(max(\"version\")).collect()[0][0]\n",
    "    starting_version = latest_version - 1 if latest_version > 0 else 0\n",
    "\n",
    "    df_bronze_changes = spark.read.format(\"delta\") \\\n",
    "        .option(\"readChangeFeed\", \"true\") \\\n",
    "        .option(\"startingVersion\", starting_version) \\\n",
    "        .table(bronze_table)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read bronze table with CDF: {e}\")\n",
    "    raise\n",
    "\n",
    "# CONVERT TO DF OBJECT\n",
    "silver_existing_df = spark.read.format(\"delta\").table(silver_table)\n",
    "\n",
    "# FILTER FOR INSERTS WITH EXPLICIT COL SELECTION\n",
    "df_silver_input = df_bronze_changes.where(\"_change_type = 'insert'\") \\\n",
    "    .select(\"Song\", \"Date\", \"Artist\", \"Rank\") \\\n",
    "    .drop(\"_change_type\", \"_commit_version\")\n",
    "\n",
    "# FILTER ONLY TOP 10\n",
    "df_silver = df_silver_input.filter(\"Rank BETWEEN 1 AND 10\")\n",
    "\n",
    "# ADD ROW NUMBERS TO DF SILVER (NEW RECORDS ONLY) FOR BATCHING\n",
    "window = Window.orderBy(\"Date\", \"Rank\", \"Song\", \"Artist\")\n",
    "df_silver = df_silver.withColumn(\"rn\", row_number().over(window))\n",
    "\n",
    "# ADD NEW COLUMNS FOR API DATA ENRICHMENT\n",
    "df_silver = df_silver \\\n",
    "    .withColumn(\"processed_date\", current_timestamp()) \\\n",
    "    .withColumn(\"Image_URL\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"Duration\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Explicit\", lit(None).cast(BooleanType())) \\\n",
    "    .withColumn(\"Song_Release_Date\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"Track_Number\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Danceability\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Energy\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Key\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Loudness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Mode\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Speechiness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Acousticness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Instrumentalness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Liveness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Valence\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Tempo\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Track_ID\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"Artist_ID\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"In Spotify API\", lit(False))\n",
    "\n",
    "# SPOTIFY TOKEN FUNCTION\n",
    "def get_spotify_token(client_id, client_secret):\n",
    "    url = \"https://accounts.spotify.com/api/token\"\n",
    "    payload = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        response.raise_for_status()\n",
    "        token_data = response.json()\n",
    "        return token_data.get(\"access_token\"), token_data.get(\"expires_in\", 3600)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error getting Spotify token: {e}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        raise\n",
    "\n",
    "# OBTAIN INITIAL SPOTIFY TOKEN\n",
    "print(\"Getting Spotify Creds\")\n",
    "client_id = \"id\"\n",
    "client_secret = \"secret\"\n",
    "access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "token_timestamp = time.time()\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "print(f\"token acquired: {access_token}\")\n",
    "\n",
    "# DEFINE VARIABLES FOR DATA ENRICHMENT LOOP\n",
    "BATCH_SIZE = 10\n",
    "total = df_silver.count()\n",
    "\n",
    "if total > 0:\n",
    "    print(f\"Enriching {total} new top-10 rows in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "    # CACHE EXISTING SILVER DATA\n",
    "    silver_existing = spark.table(silver_table) \\\n",
    "        .filter(col(\"In Spotify API\").isNotNull()) \\\n",
    "        .select(\"Song\", \"Artist\", \"Track_ID\", \"Artist_ID\", \"Duration\", \"Explicit\",\n",
    "                \"Song_Release_Date\", \"Track_Number\", \"Danceability\", \"Energy\",\n",
    "                \"Key\", \"Loudness\", \"Mode\", \"Speechiness\", \"Acousticness\", \"Instrumentalness\",\n",
    "                \"Liveness\", \"Valence\", \"Tempo\", \"Image_URL\", \"In Spotify API\") \\\n",
    "        .collect()\n",
    "\n",
    "    silver_dict = { (r.Song, r.Artist): r.asDict() for r in silver_existing }\n",
    "\n",
    "    search_url = \"https://api.spotify.com/v1/search\"\n",
    "    all_rows = df_silver.orderBy(\"rn\").collect()\n",
    "\n",
    "    # USED IN THE LOOP FOR REMOVING INFORMATION IN PARENTHESIS IN TRACK NAMES WHICH CAN CAUSE SPOTIFY SEARCH API TO FAIL TO LOCATE A TRACK\n",
    "    pattern = r'\\((.*?)\\)'\n",
    "\n",
    "    for i in range(0, total, BATCH_SIZE):\n",
    "        # REFRESH SPOTIFY TOKEN IF APPROACHING EXPIRATION\n",
    "        if time.time() - token_timestamp > token_expiry - 10:\n",
    "            print(\"Refreshing Spotify token\")\n",
    "            access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "            headers = {\"Authorization Auth acquired\": f\"Bearer {access_token}\"}\n",
    "            token_timestamp = time.time()\n",
    "\n",
    "        rows = all_rows[i:i + BATCH_SIZE]\n",
    "        print(f\"Batch {i//BATCH_SIZE + 1}: {len(rows)} rows\")\n",
    "        \n",
    "        # SET UP LOOP VARIABLES\n",
    "        rows = [row.asDict() for row in rows]\n",
    "        enriched_rows = []\n",
    "\n",
    "        # LOOP THROUGH EACH TRACK IN THE BATCH\n",
    "        for row in rows:\n",
    "            song = row[\"Song\"]\n",
    "            artist = row[\"Artist\"]\n",
    "            key = (song, artist)\n",
    "\n",
    "            # REFRESH TOKEN IF APPROACHING EXPIRATION\n",
    "            if time.time() - token_timestamp > token_expiry - 60:\n",
    "                print(f\"Token near expiry. Refreshing before calling API for: {song} - {artist}\")\n",
    "                access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "                headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "                token_timestamp = time.time()\n",
    "                print(f\"New token acquired, expires in {token_expiry} seconds.\")\n",
    "\n",
    "            # CHECK IF TRACK ALREADY EXISTS FROM A PREVIOUS WEEKLY TOP 10 AND \"In Spotify API\" HAS A VALUE (MEANING IT HAS BEEN PROCESSED BEFORE). IF IT DOES, COPY API ENRICHMENT VALUES OVER AND UPDATE WHERE APPROPRIATE.\n",
    "            if key in silver_dict and silver_dict[key][\"In Spotify API\"]:\n",
    "                print(f\"Reusing cached data for: {song} - {artist}\")\n",
    "                cached = silver_dict[key].copy()\n",
    "                cached.update({\n",
    "                    \"Song\": row[\"Song\"], \"Date\": row[\"Date\"], \"Artist\": row[\"Artist\"],\n",
    "                    \"Rank\": row[\"Rank\"],\n",
    "                    \"processed_date\": row[\"processed_date\"], \"In Spotify API\": True\n",
    "                })\n",
    "                enriched_rows.append(cached)\n",
    "                continue\n",
    "\n",
    "            # IF TRACK IS NOT IN SILVER YET, PREPARE TO CALL APIs TO PROVIDE INFORMATION ON THE TRACK\n",
    "            print(f\"Calling Spotify search for: {song} - {artist}\")\n",
    "\n",
    "            # SONG TITLES MAY HAVE WORDS IN PARENTHESES THAT CAN CAUSE THE SPOTIFY SEARCH TO NOT IDENTIFY THE SONG. REMOVE THE WORDS IN PARENTHESES. \n",
    "            match = re.search(pattern, song)\n",
    "\n",
    "            # DEFINING NEW VAR song_query SO THAT song IS NOT MODIFIED\n",
    "            song_query = song\n",
    "            if match:\n",
    "                print(f\"removing parenteses from song: {song_query}\")\n",
    "                trimmed_song_query = re.sub(r'\\([^)]*\\)', '', song_query).rstrip()\n",
    "                print(f\"removed parentheses from song. Now searching for {trimmed_song_query}\")\n",
    "                \n",
    "                # THERE MAY BE A CASE WHERE AN ENTIRE SONGNAME IS IN PARENTHESES, IN WHICH CASE THE SONG NAME SHOULD NOT HAVE THE PARENTHESES OR CONTENT BETWEEN PARENTHESES REMOVED FOR THE SEARCH\n",
    "                if len(trimmed_song_query) > 0:\n",
    "                    song_query = trimmed_song_query\n",
    "\n",
    "            query = f\"track:{song_query} artist:{artist}\"\n",
    "            params = {\"q\": query, \"type\": \"track\", \"limit\": 1}\n",
    "\n",
    "            # INTIALIZE VALUES FOR ENRICHED ROW\n",
    "            enriched_row = {\n",
    "                \"Song\": song, \"Date\": row[\"Date\"], \"Artist\": artist, \"Rank\": row[\"Rank\"],\n",
    "                \"processed_date\": row[\"processed_date\"],\n",
    "                \"Image_URL\": None, \"Duration\": None, \"Explicit\": None, \"Song_Release_Date\": None,\n",
    "                \"Track_Number\": None, \"Danceability\": None, \"Energy\": None,\n",
    "                \"Key\": None, \"Loudness\": None, \"Mode\": None, \"Speechiness\": None,\n",
    "                \"Acousticness\": None, \"Instrumentalness\": None, \"Liveness\": None,\n",
    "                \"Valence\": None, \"Tempo\": None, \"Track_ID\": None, \"Artist_ID\": None,\n",
    "                \"In Spotify API\": False\n",
    "            }\n",
    "\n",
    "            api_success = False\n",
    "            for attempt in range(2):  # TWO TRIES ALLOTTED SO IT WILL TRY AGAIN IF 401 ERROR (MEANING A TOKEN ISSUE)\n",
    "                try:\n",
    "                    response = requests.get(search_url, headers=headers, params=params, timeout=10)\n",
    "                    \n",
    "                    if response.status_code == 401:\n",
    "                        print(f\"401 Unauthorized on attempt {attempt + 1}. Forcing token refresh...\")\n",
    "                        access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "                        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "                        token_timestamp = time.time()\n",
    "                        print(f\"Refreshed token. Retrying...\")\n",
    "                        continue  # AFTER GETTING FRESH TOKEN, TRY AGAIN\n",
    "\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "\n",
    "                    # IF THERE IS DATA FOR THE TRACK FROM THE SPOTIFY API..\n",
    "                    if data[\"tracks\"][\"items\"]:\n",
    "                        track = data[\"tracks\"][\"items\"][0]\n",
    "                        track_id = track[\"id\"]\n",
    "                        print(f\"Spotify API provided track id: {track_id}\")\n",
    "\n",
    "                        # .. CAN INCLUDE SPOTIFY API VALUES IN THE ENRICHED ROW LOOPING VARIABLE TO LATER APPEND TO THE TRACK IN SILVER..\n",
    "                        enriched_row.update({\n",
    "                            \"Track_ID\": track_id,\n",
    "                            \"Artist_ID\": track[\"artists\"][0][\"id\"] if track[\"artists\"] else None,\n",
    "                            \"Image_URL\": track[\"album\"][\"images\"][0][\"url\"] if track[\"album\"][\"images\"] else None,\n",
    "                            \"Duration\": track[\"duration_ms\"],\n",
    "                            \"Explicit\": track[\"explicit\"],\n",
    "                            \"Song_Release_Date\": track[\"album\"][\"release_date\"],\n",
    "                            \"Track_Number\": track[\"track_number\"],\n",
    "                            \"In Spotify API\": True\n",
    "                        })\n",
    "\n",
    "                        # .. ALSO CAN TRY TO CALL RECCOBEATS API FOR ADDITIONAL INFORMATION ON THE TRACK\n",
    "                        try:\n",
    "                            print(f\"Calling Reccobeats API for: {song} - {artist}\")\n",
    "                            url = f\"https://api.reccobeats.com/v1/track?ids={track_id}\"\n",
    "                            recco_response = requests.get(url, headers={'Accept': 'application/json'}, timeout=10)\n",
    "                            recco_response.raise_for_status()\n",
    "                            recco_id = recco_response.json()[\"content\"][0][\"id\"] # RECCOBEATS ID IS DIFFERENT THAN SPOTIFY ID\n",
    "                            print(\"Reccobeats API success\")\n",
    "\n",
    "                            # IF RECCOBEATS API CALL IS SUCCESSFUL, CAN INCLUDE RECCOBEATS API VALUES IN THE ENRICHED ROW LOOPING VARIABLE TO LATER APPEND TO THE TRACK IN SILVER.\n",
    "                            feat_url = f\"https://api.reccobeats.com/v1/track/{recco_id}/audio-features\"\n",
    "                            feat = requests.get(feat_url, headers={'Accept': 'application/json'}, timeout=10).json()\n",
    "                            enriched_row.update({\n",
    "                                \"Danceability\":      feat.get(\"danceability\"),\n",
    "                                \"Energy\":            feat.get(\"energy\"),\n",
    "                                \"Key\":               feat.get(\"key\"),\n",
    "                                \"Loudness\":          feat.get(\"loudness\"),\n",
    "                                \"Mode\":              feat.get(\"mode\"),\n",
    "                                \"Speechiness\":       feat.get(\"speechiness\"),\n",
    "                                \"Acousticness\":      feat.get(\"acousticness\"),\n",
    "                                \"Instrumentalness\":  feat.get(\"instrumentalness\"),\n",
    "                                \"Liveness\":          feat.get(\"liveness\"),\n",
    "                                \"Valence\":           feat.get(\"valence\"),\n",
    "                                \"Tempo\":             feat.get(\"tempo\")\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(f\"Reccobeats failed for {track_id}: {e}\")\n",
    "                            # IF RECCOBEATS FAILED, PROCEED WITHOUT ENRICHING THE ROW\n",
    "\n",
    "                    else:\n",
    "                        print(f\"No tracks found for {song} - {artist}\")\n",
    "                        enriched_row[\"In Spotify API\"] = False\n",
    "                    \n",
    "                    break\n",
    "\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"API error (attempt {attempt + 1}) for {song} - {artist}: {e}\")\n",
    "                    if response and hasattr(response, 'text'):\n",
    "                        print(f\"Response: {response.text}\")\n",
    "                    if attempt == 1:\n",
    "                        enriched_row[\"In Spotify API\"] = False\n",
    "                    time.sleep(1)\n",
    "\n",
    "            # INCLUDE ROW FOR ENRICHED TRACK DATA IN CURRENT BATCH OF ENRICHED ROWS\n",
    "            enriched_rows.append(enriched_row)\n",
    "\n",
    "            # ADD TO SILVER DICT TO ALLOW FOR REUSING APPROPRIATE TRACK DATA IF TRACK IS IN A TOP 10 LIST IN A SUBSEQUENT WEEK\n",
    "            silver_dict[key] = enriched_row\n",
    "\n",
    "            time.sleep(1)  # DELAY TO PREVENT SPOTIFY API LIMIT       \n",
    "\n",
    "        print(f\"Enriched {len(enriched_rows)} rows for batch {i//BATCH_SIZE + 1}\")\n",
    "\n",
    "        # APPEND BATCH OF 10 ROWS\n",
    "        if enriched_rows:\n",
    "            spark.createDataFrame(enriched_rows, schema=silver_schema).write \\\n",
    "                .format(\"delta\").mode(\"append\").saveAsTable(silver_table)\n",
    "            print(f\"  Appended batch {i//BATCH_SIZE + 1}\")\n",
    "        else:\n",
    "            print(f\"  No rows to append for batch {i//BATCH_SIZE + 1}\")\n",
    "\n",
    "else:\n",
    "    print(\"No new songs to append.\")\n",
    "\n",
    "# PRINT FINAL COUNT OF SILVER TABLE\n",
    "final_count = spark.table(silver_table).count()\n",
    "print(f\"Silver table final row count: {final_count}\")\n",
    "\n",
    "# OPTIMIZE AND Z ORDER\n",
    "_ = spark.sql(f\"OPTIMIZE {silver_table} ZORDER BY (Date, Rank)\")\n",
    "print(\"Silver optimized with ZORDER by (Date, Rank). Ingestion complete.\")\n",
    "\n",
    "# TABLE TO SHOW SPOTIFY SUCCESS RATE\n",
    "print(\"Spotify enrichment status:\")\n",
    "spark.table(silver_table).groupBy(\"In Spotify API\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9d59b5-279d-46bf-ac47-5f1e5a7da679",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Image_URL\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1762485909515}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_table = \"silver.hot100_clean\"\n",
    "silver_table = spark.read.table(silver_table)\n",
    "display(silver_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac94d99c-d7f7-47f2-979d-2c2cc0ca75b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM silver.hot100_clean;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02db91b1-748a-4bb9-9d18-57585b250e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- BEFORE CREATING GOLD TABLES, FIRST MUST ALTER SILVER TABLE TO ENABLE COLUMN MAPPING\n",
    "\n",
    "ALTER TABLE silver.hot100_clean \n",
    "SET TBLPROPERTIES (\n",
    "  'delta.columnMapping.mode' = 'name',\n",
    "  'delta.minReaderVersion' = '2',\n",
    "  'delta.minWriterVersion' = '5'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58955bbc-98eb-4c88-a89a-82f09bd2e938",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Image_URL\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1762640892794}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM silver.hot100_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b5d8e4-08ea-4ec2-8dd4-3056264b6ef7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Image_URL\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1762486599146}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER MANAGED TABLE #1, WEEKLY TOP 10\n",
    "-- This table contains the cleaned Top 10 data.\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS gold;\n",
    "USE SCHEMA gold;\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.weekly_top10_snapshot\n",
    "USING DELTA\n",
    "PARTITIONED BY (year)\n",
    "TBLPROPERTIES (\n",
    "  'delta.columnMapping.mode' = 'name',\n",
    "  'delta.minReaderVersion' = '2',\n",
    "  'delta.minWriterVersion' = '5'\n",
    ")\n",
    "AS\n",
    "SELECT *, YEAR(Date) AS year\n",
    "FROM silver.hot100_clean;\n",
    "\n",
    "SELECT COUNT(*) AS row_count FROM gold.weekly_top10_snapshot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03869f2c-ab7e-4993-9f7a-84e9c8120cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER MANAGED TABLE #2, ARTIST PERFORMANCE\n",
    "-- This table contains Top 10 data aggregated by artist to get insight on individual artist performance throughout history.\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.artist_performance\n",
    "USING DELTA\n",
    "AS\n",
    "WITH unique_songs AS (\n",
    "  SELECT DISTINCT\n",
    "    Artist,\n",
    "    Song,\n",
    "    Track_ID,\n",
    "    Duration,\n",
    "    Explicit,\n",
    "    Danceability,\n",
    "    Energy,\n",
    "    Tempo,\n",
    "    `Key`,\n",
    "    Mode\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Track_ID IS NOT NULL\n",
    "),\n",
    "song_counts AS (\n",
    "  SELECT\n",
    "    Artist,\n",
    "    Song,\n",
    "    COUNT(*) AS weeks_in_top10\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  GROUP BY Artist, Song\n",
    "),\n",
    "artist_song_summary AS (\n",
    "  SELECT\n",
    "    us.Artist,\n",
    "    COUNT(DISTINCT us.Song) AS total_unique_songs,\n",
    "    SUM(CASE WHEN sc.weeks_in_top10 >= 1 THEN 1 ELSE 0 END) AS total_top10_songs,\n",
    "    SUM(CASE WHEN MIN_Rank = 1 THEN 1 ELSE 0 END) AS number_ones,\n",
    "    SUM(CASE WHEN us.Explicit THEN 1 ELSE 0 END) AS explicit_songs,\n",
    "    AVG(us.Duration) AS avg_duration_ms,\n",
    "    AVG(us.Tempo) AS avg_tempo,\n",
    "    AVG(us.`Key`) AS avg_key,\n",
    "    AVG(us.Mode) AS avg_mode,\n",
    "    SUM(sc.weeks_in_top10) AS total_weeks_with_top10_entry\n",
    "  FROM unique_songs us\n",
    "  JOIN (\n",
    "    SELECT Artist, Song, MIN(Rank) AS MIN_Rank\n",
    "    FROM gold.weekly_top10_snapshot\n",
    "    GROUP BY Artist, Song\n",
    "  ) peak ON us.Artist = peak.Artist AND us.Song = peak.Song\n",
    "  JOIN song_counts sc ON us.Artist = sc.Artist AND us.Song = sc.Song\n",
    "  GROUP BY us.Artist\n",
    ")\n",
    "SELECT\n",
    "  Artist,\n",
    "  total_unique_songs,\n",
    "  total_top10_songs,\n",
    "  number_ones,\n",
    "  explicit_songs,\n",
    "  avg_duration_ms,\n",
    "  avg_tempo,\n",
    "  avg_key,\n",
    "  avg_mode,\n",
    "  total_weeks_with_top10_entry\n",
    "FROM artist_song_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7505ca39-8822-445b-8380-fbf092262d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS row_count FROM gold.artist_performance;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de1a38aa-7c11-448b-86ce-4a7666e757d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER MANAGED TABLE #3, SONG SPECIFIC DATA\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.song_lifecycle\n",
    "USING DELTA\n",
    "AS\n",
    "WITH ranked AS (\n",
    "  SELECT\n",
    "    Song, Artist, Date, Rank,\n",
    "    ROW_NUMBER() OVER (PARTITION BY Song, Artist ORDER BY Date) AS week_num,\n",
    "    MIN(Rank) OVER (PARTITION BY Song, Artist) AS peak_rank\n",
    "  FROM gold.weekly_top10_snapshot\n",
    ")\n",
    "SELECT\n",
    "  Song, Artist,\n",
    "  MIN(Date) AS debut_date,\n",
    "  MAX(Date) AS last_date,\n",
    "  peak_rank,\n",
    "  COUNT(*) AS total_weeks,\n",
    "  MIN(week_num) FILTER (WHERE Rank = peak_rank) AS weeks_to_peak\n",
    "FROM ranked\n",
    "GROUP BY Song, Artist, peak_rank;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573e9bfe-013f-49be-824f-31233bbb2134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS row_count FROM gold.song_lifecycle;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c4ff43-bf09-4d98-8c3c-b08fa471e561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER MANAGED TABLE #4, DECADE SUMMARY\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.decade_summary\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT\n",
    "  FLOOR(YEAR(Date)/10)*10 AS decade,\n",
    "  COUNT(*) AS total_songs,\n",
    "  \n",
    "  AVG(Duration) AS avg_duration_ms,\n",
    "  AVG(Danceability) AS avg_danceability,\n",
    "  AVG(Energy) AS avg_energy,\n",
    "  AVG(`Key`) AS avg_key,\n",
    "  AVG(Loudness) AS avg_loudness,\n",
    "  AVG(Mode) AS avg_mode,\n",
    "  AVG(Speechiness) AS avg_speechiness,\n",
    "  AVG(Acousticness) AS avg_acousticness,\n",
    "  AVG(Instrumentalness) AS avg_instrumentalness,\n",
    "  AVG(Liveness) AS avg_liveness,\n",
    "  AVG(Valence) AS avg_valence,\n",
    "  AVG(Tempo) AS avg_tempo,\n",
    "  AVG(CASE WHEN Explicit THEN 1.0 ELSE 0.0 END) AS pct_explicit\n",
    "\n",
    "FROM (\n",
    "  -- ENSURE 1 ROW PER UNIQUE SONG TO AVOID PERSISTENT TOP 10 TRACKS FROM SKEWING DATA\n",
    "  SELECT DISTINCT\n",
    "    Song, Artist, Track_ID,\n",
    "    Duration, Explicit, Danceability, Energy, `Key`, Loudness, Mode,\n",
    "    Speechiness, Acousticness, Instrumentalness, Liveness, Valence, Tempo,\n",
    "    Date\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Track_ID IS NOT NULL\n",
    ") unique_songs\n",
    "GROUP BY FLOOR(YEAR(Date)/10)*10\n",
    "ORDER BY decade;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d2016f-5f77-4729-baf4-2a6745361f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS row_count FROM gold.decade_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d127ee43-e3f5-4dee-919b-9ea67c235763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER MANAGED TABLE #4, ANNUAL SUMMARY\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.annual_summary\n",
    "USING DELTA\n",
    "PARTITIONED BY (year)\n",
    "AS\n",
    "SELECT\n",
    "  YEAR(Date) AS year,\n",
    "  COUNT(*) AS total_songs,\n",
    "\n",
    "  -- Audio & Metadata (averaged over unique songs only)\n",
    "  AVG(Duration) AS avg_duration_ms,\n",
    "  AVG(Danceability) AS avg_danceability,\n",
    "  AVG(Energy) AS avg_energy,\n",
    "  AVG(`Key`) AS avg_key,\n",
    "  AVG(Loudness) AS avg_loudness,\n",
    "  AVG(Mode) AS avg_mode,\n",
    "  AVG(Speechiness) AS avg_speechiness,\n",
    "  AVG(Acousticness) AS avg_acousticness,\n",
    "  AVG(Instrumentalness) AS avg_instrumentalness,\n",
    "  AVG(Liveness) AS avg_liveness,\n",
    "  AVG(Valence) AS avg_valence,\n",
    "  AVG(Tempo) AS avg_tempo,\n",
    "\n",
    "  -- % of songs that are explicit\n",
    "  AVG(CASE WHEN Explicit THEN 1.0 ELSE 0.0 END) AS pct_explicit\n",
    "\n",
    "FROM (\n",
    "  -- One row per unique song that appeared in Top 10 that year\n",
    "  SELECT DISTINCT\n",
    "    Song, Artist, Track_ID,\n",
    "    Duration, Explicit, Danceability, Energy, `Key`, Loudness, Mode,\n",
    "    Speechiness, Acousticness, Instrumentalness, Liveness, Valence, Tempo,\n",
    "    Date\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Track_ID IS NOT NULL\n",
    ") unique_songs\n",
    "GROUP BY YEAR(Date)\n",
    "ORDER BY year;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f1b9ab-6208-4bd8-83b3-909e6322261c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS row_count FROM gold.annual_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5fbcd7-3db6-48ad-b57b-584984561c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER MANAGED TABLE #5, SONGS AT #1 FOR CONSECUTIVE WEEKS\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.reign_tracker\n",
    "USING DELTA\n",
    "AS\n",
    "WITH ranked AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    LAG(Date) OVER (PARTITION BY Song, Artist ORDER BY Date) AS prev_date\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Rank = 1\n",
    "),\n",
    "streaks AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    CASE \n",
    "      WHEN prev_date IS NULL OR DATEDIFF(Date, prev_date) > 7 \n",
    "      THEN 1 \n",
    "      ELSE 0 \n",
    "    END AS new_streak\n",
    "  FROM ranked\n",
    "),\n",
    "streak_groups AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    SUM(new_streak) OVER (PARTITION BY Song, Artist ORDER BY Date) AS streak_id\n",
    "  FROM streaks\n",
    ")\n",
    "SELECT\n",
    "  Song,\n",
    "  Artist,\n",
    "  MIN(Date) AS start_date,\n",
    "  MAX(Date) AS end_date,\n",
    "  COUNT(*) AS streak_weeks\n",
    "FROM streak_groups\n",
    "GROUP BY Song, Artist, streak_id\n",
    "HAVING COUNT(*) >= 1\n",
    "ORDER BY streak_weeks DESC;\n",
    "\n",
    "SELECT * FROM gold.reign_tracker;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c5e27e-ac63-46eb-b0c8-8a5c52965de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- How many weeks was each song #1? (ALL TIME)\n",
    "SELECT Song, Artist, COUNT(*) AS weeks_at_1\n",
    "FROM gold.weekly_top10_snapshot\n",
    "WHERE Rank = 1\n",
    "GROUP BY Song, Artist\n",
    "ORDER BY weeks_at_1 DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a809b5bd-b96e-4184-814a-174aa00511d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER MANAGED TABLE #6, SONGS IN TOP 10 FOR CONSECUTIVE WEEKS\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.top10_streak_tracker\n",
    "USING DELTA\n",
    "AS\n",
    "WITH ranked AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    ROW_NUMBER() OVER (PARTITION BY Song, Artist ORDER BY Date) AS rn,\n",
    "    LAG(Date) OVER (PARTITION BY Song, Artist ORDER BY Date) AS prev_date\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Rank <= 10\n",
    "),\n",
    "streaks AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    rn,\n",
    "    CASE \n",
    "      WHEN prev_date IS NULL THEN 1 -- means it was not in the previous weekly Top 10, so not on a streak\n",
    "      WHEN DATEDIFF(Date, prev_date) = 7 THEN 0  -- if DATEDIFF = 7, that means consecutive weeks so continue streak\n",
    "      ELSE 1  -- if gap in top 10 appearances, reset to 1 as it is no longer consecutive\n",
    "    END AS is_new_streak\n",
    "  FROM ranked\n",
    "),\n",
    "streak_groups AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    SUM(is_new_streak) OVER (PARTITION BY Song, Artist ORDER BY Date) AS streak_id\n",
    "  FROM streaks\n",
    ")\n",
    "SELECT\n",
    "  Song,\n",
    "  Artist,\n",
    "  MIN(Date) AS start_date,\n",
    "  MAX(Date) AS end_date,\n",
    "  COUNT(*) AS streak_weeks\n",
    "FROM streak_groups\n",
    "GROUP BY Song, Artist, streak_id\n",
    "HAVING COUNT(*) >= 1\n",
    "ORDER BY streak_weeks DESC, start_date;\n",
    "\n",
    "SELECT * FROM gold.top10_streak_tracker LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e78d68-b6aa-4d9f-8b85-b90b9e12574e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 9005887144713656,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "billboard-top-100 BRONZE SILVER GOLD",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
