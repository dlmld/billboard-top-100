{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1d8997-f415-4d7b-831e-a5dd063f4a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test only; run to clear Bronze\n",
    "# dbutils.fs.rm(\"/mnt/billboard_mount/bronze_delta/hot100_raw\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2256b028-b5a2-45bf-a0aa-2ee3f5dfd296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Test only; run to clear Bronze\n",
    "-- DROP TABLE IF EXISTS bronze.hot100_raw;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b08665-1f43-46a8-8191-0b907a133f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test only; run to clear Bronze\n",
    "# spark.sql(\"DROP TABLE IF EXISTS spark_catalog.silver.hot100_clean\")\n",
    "# dbutils.fs.rm(\"/mnt/billboard_mount/silver/hot100_clean\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef3a62a-8de8-459e-946e-8fff09ef0f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test only; run to clear silver\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_table}\")\n",
    "# dbutils.fs.rm(silver_delta_path, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fe86fa-ec37-455d-ad0d-a26eccf7f37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# BRONZE\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "import urllib.parse\n",
    "\n",
    "# CONFIGURE AWS MOUNT\n",
    "aws_access_key = \"KEY\"\n",
    "aws_secret_key = \"SECRET\"\n",
    "aws_bucket_name = \"billboard-hot100-project\"\n",
    "mount_name = \"billboard_mount\"\n",
    "\n",
    "# CONFIGURE BRONZE PATHS\n",
    "source_csv_path = f\"/mnt/{mount_name}/bronze/hot100_delta_*.csv\"\n",
    "bronze_table = \"bronze.hot100_raw\"\n",
    "bronze_delta_path = f\"/mnt/{mount_name}/bronze_delta/hot100_raw\"\n",
    "\n",
    "# VARIABLES FOR CONTROLLING PROCESSING\n",
    "MAX_ROWS = 1500  # Limit rows processed for testing purposes\n",
    "IS_FIRST_RUN = not DeltaTable.isDeltaTable(spark, bronze_delta_path) # On the first run, all historical data is ran \n",
    "\n",
    "# MOUNT AWS S3\n",
    "encoded_secret_key = urllib.parse.quote(aws_secret_key, \"\")\n",
    "if not any(m.mountPoint == f\"/mnt/{mount_name}\" for m in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount(f\"s3a://{aws_access_key}:{encoded_secret_key}@{aws_bucket_name}\", f\"/mnt/{mount_name}\")\n",
    "    print(f\"Mounted S3 bucket at /mnt/{mount_name}\")\n",
    "else:\n",
    "    print(f\"Mount /mnt/{mount_name} already exists\")\n",
    "\n",
    "# DEFINE SCHEMA OF INGEST\n",
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Song\", StringType(), True),\n",
    "    StructField(\"Artist\", StringType(), True),\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"Last Week\", IntegerType(), True),\n",
    "    StructField(\"Peak Position\", IntegerType(), True),\n",
    "    StructField(\"Weeks in Charts\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# FIRST RUN: INITIALIZE BRONZE TABLE ON FIRST RUN\n",
    "if IS_FIRST_RUN:\n",
    "    print(\"First run: initializing Bronze delta table\")\n",
    "    \n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(source_csv_path) \\\n",
    "        .withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "        .limit(MAX_ROWS)\n",
    "    \n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "        .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "        .save(bronze_delta_path)\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {}\n",
    "        USING DELTA LOCATION '{}'\n",
    "        TBLPROPERTIES (\n",
    "            delta.enableChangeDataFeed = true,\n",
    "            delta.columnMapping.mode = 'name'\n",
    "        )\n",
    "    \"\"\".format(bronze_table, bronze_delta_path))\n",
    "    \n",
    "    _ = spark.sql(\"OPTIMIZE {} ZORDER BY (Date, Rank)\".format(bronze_table))\n",
    "    print(f\"Initialized Bronze table with {df.count()} rows (MAX_ROWS = {MAX_ROWS})\")\n",
    "\n",
    "# LOAD CSV FILES FROM S3\n",
    "all_csv_files = [\n",
    "    f.path for f in dbutils.fs.ls(f\"/mnt/{mount_name}/bronze/\")\n",
    "    if f.name.startswith(\"hot100_delta_\") and f.name.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# COLLECT FILES ALREADY IN BRONZE\n",
    "try:\n",
    "    processed_files = spark.sql(\"SELECT DISTINCT input_file_name() FROM {}\".format(bronze_table)) \\\n",
    "        .rdd.map(lambda x: x[0]).collect()\n",
    "except:\n",
    "    processed_files = []\n",
    "\n",
    "# LOCATE FILES NOT YET IN BRONZE\n",
    "new_files = [f for f in all_csv_files if f not in processed_files]\n",
    "\n",
    "# ENSURE THERE ARE NEW FILES IN INCREMENTAL RUNS\n",
    "if not new_files:\n",
    "    df_new = spark.createDataFrame([], schema)\n",
    "    print(\"No new files found\")\n",
    "else:\n",
    "    print(f\"Found {len(new_files)} new file(s)\")\n",
    "    df_new = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(new_files) \\\n",
    "        .withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # FILTER ONLY NEW RECORDS USING LEFT ANTI JOIN TO EXCLUSE EXISTING RECORDS\n",
    "    existing_dates = spark.table(bronze_table).select(\"Date\").distinct()\n",
    "    df_new = df_new.join(existing_dates, \"Date\", \"left_anti\")\n",
    "    \n",
    "    if df_new.count() == 0:\n",
    "        print(\"No new dates found in new files. Skipping append.\")\n",
    "        df_new = spark.createDataFrame([], schema)\n",
    "    else:\n",
    "        new_date_count = df_new.select(\"Date\").distinct().count()\n",
    "        print(f\"Found {new_date_count} new dates in new files\")\n",
    "        \n",
    "        # FOR TESTING: CAP TO MAX_ROWS TO REDUCE UNNECESSARY PROCESSING\n",
    "        spark.sql(\"REFRESH TABLE {}\".format(bronze_table))\n",
    "        current_count = spark.table(bronze_table).count()\n",
    "        print(f\"Current rows: {current_count}, MAX_ROWS: {MAX_ROWS}\")\n",
    "        \n",
    "        # CALCULATE NUMBER OF ROWS TO ADD BASED ON MAX_ROWS VALUE\n",
    "        if current_count >= MAX_ROWS:\n",
    "            df_new = spark.createDataFrame([], schema)\n",
    "            print(\"MAX_ROWS reached. Skipping append.\")\n",
    "        else:\n",
    "            to_take = MAX_ROWS - current_count\n",
    "            df_new = df_new.limit(to_take)\n",
    "            print(f\"Taking {to_take} rows to reach MAX_ROWS={MAX_ROWS}\")\n",
    "\n",
    "# WITH NEW ROWS ISOLATED, APPEND TO BRONZE\n",
    "if df_new.count() > 0:\n",
    "    df_new.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_table)\n",
    "    print(f\"Appended {df_new.count()} new rows\")\n",
    "else:\n",
    "    print(\"No new rows\")\n",
    "\n",
    "# OPTIMIZE DELTA TABLE\n",
    "_ = spark.sql(\"OPTIMIZE {} ZORDER BY (Date, Rank)\".format(bronze_table))\n",
    "print(\"Bronze ingestion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f398a0b0-1ffa-412a-b567-ff3fbcf79626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SILVER\n",
    "\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, max, row_number\n",
    "from pyspark.sql.types import StringType, IntegerType, BooleanType, FloatType, DateType, TimestampType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "# DEFINE PATHS AND TABLES\n",
    "bronze_table = \"bronze.hot100_raw\"\n",
    "silver_table = \"silver.hot100_clean\"\n",
    "silver_delta_path = \"/mnt/billboard_mount/silver/hot100_clean\"\n",
    "\n",
    "# DEFINE SILVER SCHEMA\n",
    "silver_schema = StructType([\n",
    "    StructField(\"Song\", StringType(), True),\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Artist\", StringType(), True),\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"processed_date\", TimestampType(), True),\n",
    "    StructField(\"Image_URL\", StringType(), True),\n",
    "    StructField(\"Duration\", IntegerType(), True),\n",
    "    StructField(\"Explicit\", BooleanType(), True),\n",
    "    StructField(\"Song_Release_Date\", StringType(), True),\n",
    "    StructField(\"Track_Number\", IntegerType(), True),\n",
    "    StructField(\"Danceability\", FloatType(), True),\n",
    "    StructField(\"Energy\", FloatType(), True),\n",
    "    StructField(\"Key\", IntegerType(), True),\n",
    "    StructField(\"Loudness\", FloatType(), True),\n",
    "    StructField(\"Mode\", IntegerType(), True),\n",
    "    StructField(\"Speechiness\", FloatType(), True),\n",
    "    StructField(\"Acousticness\", FloatType(), True),\n",
    "    StructField(\"Instrumentalness\", FloatType(), True),\n",
    "    StructField(\"Liveness\", FloatType(), True),\n",
    "    StructField(\"Valence\", FloatType(), True),\n",
    "    StructField(\"Tempo\", FloatType(), True),\n",
    "    StructField(\"Track_ID\", StringType(), True),\n",
    "    StructField(\"Artist_ID\", StringType(), True),\n",
    "    StructField(\"In Spotify API\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# CREATE SCHEMA FOR SILVER (FIRST RUN)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "\n",
    "# DEFINE SCHEMA FOR SILVER\n",
    "# EXCLUDING COLUMNS FROM BRONZE: PEAK POSITION, LAST WEEK, WEEKS IN CHARTS. THESE VALUES IN THE DATA SOURCE ARE BASED ON THE TOP 100, IN THIS CASE JUST INTERESTED IN TRACKS PLACEMENT IN TOP 10 TO LIMIT PROCESSING. THEY WILL BE RECALCULATED IN GOLD LAYER.\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {} (\n",
    "            Song STRING,\n",
    "            Date DATE,\n",
    "            Artist STRING,\n",
    "            Rank INT,\n",
    "            processed_date TIMESTAMP,\n",
    "            Image_URL STRING,\n",
    "            Duration INT,\n",
    "            Explicit BOOLEAN,\n",
    "            Song_Release_Date STRING,\n",
    "            Track_Number INT,\n",
    "            Danceability FLOAT,\n",
    "            Energy FLOAT,\n",
    "            `Key` INT,\n",
    "            Loudness FLOAT,\n",
    "            Mode INT,\n",
    "            Speechiness FLOAT,\n",
    "            Acousticness FLOAT,\n",
    "            Instrumentalness FLOAT,\n",
    "            Liveness FLOAT,\n",
    "            Valence FLOAT,\n",
    "            Tempo FLOAT,\n",
    "            Track_ID STRING,\n",
    "            Artist_ID STRING,\n",
    "            `In Spotify API` BOOLEAN\n",
    "        )\n",
    "        USING DELTA\n",
    "        LOCATION '{}'\n",
    "        TBLPROPERTIES (\n",
    "            delta.enableChangeDataFeed = true,\n",
    "            delta.columnMapping.mode = 'name'\n",
    "        )\n",
    "    \"\"\".format(silver_table, silver_delta_path))\n",
    "    print(f\"Silver table {silver_table} checked/created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating silver table: {e}\")\n",
    "    raise\n",
    "\n",
    "# USE CHANGE DATA FEED (CDF) TO READ ONLY CHANGES TO THE BRONZE TABLE FOR INCREMENTAL PROCESSING\n",
    "try:\n",
    "    spark.sql(f\"REFRESH TABLE {bronze_table}\")\n",
    "    latest_version = spark.sql(\"DESCRIBE HISTORY {}\".format(bronze_table)).select(max(\"version\")).collect()[0][0]\n",
    "    starting_version = latest_version - 1 if latest_version > 0 else 0\n",
    "\n",
    "    df_bronze_changes = spark.read.format(\"delta\") \\\n",
    "        .option(\"readChangeFeed\", \"true\") \\\n",
    "        .option(\"startingVersion\", starting_version) \\\n",
    "        .table(bronze_table)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read bronze table with CDF: {e}\")\n",
    "    raise\n",
    "\n",
    "# CONVERT TO DF OBJECT\n",
    "silver_existing_df = spark.read.format(\"delta\").table(silver_table)\n",
    "\n",
    "# FILTER FOR INSERTS WITH EXPLICIT COL SELECTION\n",
    "df_silver_input = df_bronze_changes.where(\"_change_type = 'insert'\") \\\n",
    "    .select(\"Song\", \"Date\", \"Artist\", \"Rank\") \\\n",
    "    .drop(\"_change_type\", \"_commit_version\")\n",
    "\n",
    "# FILTER ONLY TOP 10\n",
    "df_silver = df_silver_input.filter(\"Rank BETWEEN 1 AND 10\")\n",
    "\n",
    "# ADD ROW NUMBERS TO DF SILVER (NEW RECORDS ONLY) FOR BATCHING\n",
    "window = Window.orderBy(\"Date\", \"Rank\", \"Song\", \"Artist\")\n",
    "df_silver = df_silver.withColumn(\"rn\", row_number().over(window))\n",
    "\n",
    "# ADD NEW COLUMNS FOR API DATA ENRICHMENT\n",
    "df_silver = df_silver \\\n",
    "    .withColumn(\"processed_date\", current_timestamp()) \\\n",
    "    .withColumn(\"Image_URL\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"Duration\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Explicit\", lit(None).cast(BooleanType())) \\\n",
    "    .withColumn(\"Song_Release_Date\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"Track_Number\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Danceability\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Energy\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Key\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Loudness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Mode\", lit(None).cast(IntegerType())) \\\n",
    "    .withColumn(\"Speechiness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Acousticness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Instrumentalness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Liveness\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Valence\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Tempo\", lit(None).cast(FloatType())) \\\n",
    "    .withColumn(\"Track_ID\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"Artist_ID\", lit(None).cast(StringType())) \\\n",
    "    .withColumn(\"In Spotify API\", lit(False))\n",
    "\n",
    "\n",
    "# --------------------\n",
    "\n",
    "# SPOTIFY TOKEN FUNCTION\n",
    "def get_spotify_token(client_id, client_secret):\n",
    "    url = \"https://accounts.spotify.com/api/token\"\n",
    "    payload = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        response.raise_for_status()\n",
    "        token_data = response.json()\n",
    "        return token_data.get(\"access_token\"), token_data.get(\"expires_in\", 3600)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error getting Spotify token: {e}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        raise\n",
    "\n",
    "# ---------------\n",
    "\n",
    "# FUNCTION TO CLEAN UP SONG, ARTIST NAMES TO REMOVE CHARS THAT COULD INHIBIT SPOTIFY SEARCH API.\n",
    "def query_clean(song: str, artist: str):\n",
    "    # SONG TITLES MAY HAVE WORDS IN PARENTHESES THAT CAN CAUSE THE SPOTIFY SEARCH TO NOT IDENTIFY THE SONG. REMOVE THE WORDS IN PARENTHESES.\n",
    "    pattern = r'\\((.*?)\\)'\n",
    "    match = re.search(pattern, song)\n",
    "    if match:\n",
    "        print(f\"Removing parenteses from song: {song}\")\n",
    "        trimmed_song = re.sub(r'\\([^)]*\\)', '', song).rstrip()\n",
    "        print(f\"Removed parentheses from song. Now searching for {song}\")\n",
    "        \n",
    "        # THERE MAY BE A CASE WHERE AN ENTIRE SONGNAME IS IN PARENTHESES, IN WHICH CASE THE SONG NAME SHOULD NOT HAVE THE PARENTHESES OR CONTENT BETWEEN PARENTHESES REMOVED FOR THE SEARCH\n",
    "        if len(trimmed_song) > 0:\n",
    "            song = trimmed_song\n",
    "\n",
    "    # REMOVE ANY DOUBLE QUOTATION MARKS IN THE SONG TITLE\n",
    "    song = song.replace('\"', '').strip()\n",
    "\n",
    "    # THIS ISSUE MAY OCCUR WITH SOME ARTIST NAMES TOO. REMOVE DOUBLE QUOTATIONS IN THE ARTIST NAME.\n",
    "    artist = artist.replace('\"', '').strip()\n",
    "    return song, artist\n",
    "\n",
    "# ------------------\n",
    "\n",
    "def grok_clean_query(song: str, artist: str) -> tuple:\n",
    "    prompt = f\"\"\"\n",
    "You are a music metadata expert. Spotify's Track Search API input requires the song and artist names.\n",
    "\n",
    "Clean the provided inputs for the Spotify API search.\n",
    "\n",
    "Remove suffixes like \"with the XYZs\", \"and His Orchestra\", (Remix), quotes, \"feat (xyz)\", etc.\n",
    "Keep the core name that Spotify would recognize.\n",
    "\n",
    "For example, the track \"Quiet Village\" is sometimes listed under the artist name \"The Exotic Sounds of Martin Denny\", but Spotify Search API will only find this track if the artist name is just \"Martin Denny\".\n",
    "\n",
    "Another example: \"Only You\" is sometimes listed under the artist name \"Franck Pourcel's French Fiddles\", but Spotify Search API will only find this track if the artist name is just \"Frank Pourcel\"\n",
    "\n",
    "Last example: A track is listed as \"Kookie; Kookie (Lend Me Your Comb)\" is sometimes listed under \"Edward Byrnes & Connie Stevens\", but Spotify has the track under \"Connie Stevens with Edd Byrnes\".\n",
    "\n",
    "Infer what the best way to clean the inputs for the Spotify API search would be.\n",
    "\n",
    "Input:\n",
    "Song: \"{song}\"\n",
    "Artist: \"{artist}\"\n",
    "\n",
    "Return ONLY JSON:\n",
    "{{\n",
    "  \"song\": \"core song name\",\n",
    "  \"artist\": \"core artist name\"\n",
    "}}\n",
    "\"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"grok-3\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"Bearer {grok_api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(grok_url, json=payload, headers=headers, timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            data = json.loads(resp.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "            return data[\"song\"].strip(), data[\"artist\"].strip()\n",
    "        else:\n",
    "            print(f\"Grok error {resp.status_code}: {resp.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Grok call failed: {e}\")\n",
    "    return song, artist  # fallback\n",
    "\n",
    "\n",
    "# ------------------\n",
    "\n",
    "def spotify_search(query, headers, params):\n",
    "    try:\n",
    "        spotify_search_url = \"https://api.spotify.com/v1/search\"\n",
    "        response = requests.get(spotify_search_url, params=params, headers=headers, timeout=10)\n",
    "        if response.status_code == 200 and response.json()[\"tracks\"][\"items\"]:\n",
    "            data = response.json()\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Spotify search failed: {e}\")\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ------------------\n",
    "\n",
    "# OBTAIN INITIAL SPOTIFY TOKEN\n",
    "print(\"Getting Spotify Creds\")\n",
    "client_id = \"ID\"\n",
    "client_secret = \"SECRET\"\n",
    "access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "token_timestamp = time.time()\n",
    "spotify_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "print(f\"token acquired: {access_token}\")\n",
    "\n",
    "# DEFINE VARIABLES FOR DATA ENRICHMENT LOOP\n",
    "BATCH_SIZE = 10\n",
    "total = df_silver.count()\n",
    "#grok_api_key = dbutils.secrets.get(\"grok\", \"api-key\") # IF GROK KEY IS STORED IN DATABRICKS, USE THIS FOR ACCESSING KEY RATHER THAN HARDCODING\n",
    "grok_api_key = 'KEY'\n",
    "grok_url = \"https://api.x.ai/v1/chat/completions\"\n",
    "\n",
    "if total > 0:\n",
    "    print(f\"Enriching {total} new top-10 rows in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "    # CACHE EXISTING SILVER DATA\n",
    "    silver_existing = spark.table(silver_table) \\\n",
    "        .filter(col(\"In Spotify API\").isNotNull()) \\\n",
    "        .select(\"Song\", \"Artist\", \"Track_ID\", \"Artist_ID\", \"Duration\", \"Explicit\",\n",
    "                \"Song_Release_Date\", \"Track_Number\", \"Danceability\", \"Energy\",\n",
    "                \"Key\", \"Loudness\", \"Mode\", \"Speechiness\", \"Acousticness\", \"Instrumentalness\",\n",
    "                \"Liveness\", \"Valence\", \"Tempo\", \"Image_URL\", \"In Spotify API\") \\\n",
    "        .collect()\n",
    "\n",
    "    silver_dict = { (r.Song, r.Artist): r.asDict() for r in silver_existing }\n",
    "\n",
    "    all_rows = df_silver.orderBy(\"rn\").collect()\n",
    "\n",
    "    for i in range(0, total, BATCH_SIZE):\n",
    "        # REFRESH SPOTIFY TOKEN IF APPROACHING EXPIRATION\n",
    "        if time.time() - token_timestamp > token_expiry - 10:\n",
    "            print(\"Refreshing Spotify token\")\n",
    "            access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "            headers = {\"Authorization Auth acquired\": f\"Bearer {access_token}\"}\n",
    "            token_timestamp = time.time()\n",
    "\n",
    "        rows = all_rows[i:i + BATCH_SIZE]\n",
    "        print(f\"Batch {i//BATCH_SIZE + 1}: {len(rows)} rows\")\n",
    "        \n",
    "        # SET UP LOOP VARIABLES\n",
    "        rows = [row.asDict() for row in rows]\n",
    "        enriched_rows = []\n",
    "\n",
    "        # LOOP THROUGH EACH TRACK IN THE BATCH\n",
    "        for row in rows:\n",
    "            song = row[\"Song\"]\n",
    "            artist = row[\"Artist\"]\n",
    "            key = (song, artist)\n",
    "\n",
    "            # REFRESH TOKEN IF APPROACHING EXPIRATION\n",
    "            if time.time() - token_timestamp > token_expiry - 60:\n",
    "                print(f\"Token near expiry. Refreshing before calling API for: {song} - {artist}\")\n",
    "                access_token, token_expiry = get_spotify_token(client_id, client_secret)\n",
    "                headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "                token_timestamp = time.time()\n",
    "                print(f\"New token acquired, expires in {token_expiry} seconds.\")\n",
    "\n",
    "            # CHECK IF TRACK ALREADY EXISTS FROM A PREVIOUS WEEKLY TOP 10 AND \"In Spotify API\" HAS A VALUE (MEANING IT HAS BEEN PROCESSED BEFORE). IF IT DOES, COPY API ENRICHMENT VALUES OVER AND UPDATE WHERE APPROPRIATE.\n",
    "            if key in silver_dict and silver_dict[key][\"In Spotify API\"]:\n",
    "                print(f\"Reusing cached data for: {song} - {artist}\")\n",
    "                cached = silver_dict[key].copy()\n",
    "                cached.update({\n",
    "                    \"Song\": row[\"Song\"], \"Date\": row[\"Date\"], \"Artist\": row[\"Artist\"],\n",
    "                    \"Rank\": row[\"Rank\"],\n",
    "                    \"processed_date\": row[\"processed_date\"], \"In Spotify API\": True\n",
    "                })\n",
    "                enriched_rows.append(cached)\n",
    "                continue\n",
    "\n",
    "            # INTIALIZE VALUES FOR ENRICHED ROW\n",
    "            enriched_row = {\n",
    "                \"Song\": song, \"Date\": row[\"Date\"], \"Artist\": artist, \"Rank\": row[\"Rank\"],\n",
    "                \"processed_date\": row[\"processed_date\"],\n",
    "                \"Image_URL\": None, \"Duration\": None, \"Explicit\": None, \"Song_Release_Date\": None,\n",
    "                \"Track_Number\": None, \"Danceability\": None, \"Energy\": None,\n",
    "                \"Key\": None, \"Loudness\": None, \"Mode\": None, \"Speechiness\": None,\n",
    "                \"Acousticness\": None, \"Instrumentalness\": None, \"Liveness\": None,\n",
    "                \"Valence\": None, \"Tempo\": None, \"Track_ID\": None, \"Artist_ID\": None,\n",
    "                \"In Spotify API\": False\n",
    "            }\n",
    "\n",
    "            # IF TRACK IS NOT IN SILVER YET, PREPARE TO CALL APIs TO PROVIDE INFORMATION ON THE TRACK\n",
    "            print(f\"Calling Spotify search for: {song} - {artist}\")\n",
    "\n",
    "            # CALL QUERY CLEAN API, BUT DON'T OVERWRITE 'song' and 'artist' as these will still be used for writing back to the cache for potentially matching instances of the same songs in subsequent batches\n",
    "            song_query, artist_query = query_clean(song, artist)\n",
    "            query = f\"track:{song_query} artist:{artist_query}\"\n",
    "            params = {\"q\": query, \"type\": \"track\", \"limit\": 1}\n",
    "            data = spotify_search(query, spotify_headers, params)\n",
    "            print(\"Spotify search 1st attempt complete\")\n",
    "\n",
    "\n",
    "            # IF THERE IS DATA FOR THE TRACK FROM THE SPOTIFY API..\n",
    "            if data is not None and \"tracks\" in data and \"items\" in data[\"tracks\"] and len(data[\"tracks\"][\"items\"]) > 0:\n",
    "                track = data[\"tracks\"][\"items\"][0]\n",
    "                track_id = track[\"id\"]\n",
    "                print(f\"Spotify API provided track id: {track_id}\")\n",
    "\n",
    "                # .. CAN INCLUDE SPOTIFY API VALUES IN THE ENRICHED ROW LOOPING VARIABLE TO LATER APPEND TO THE TRACK IN SILVER..\n",
    "                enriched_row.update({\n",
    "                    \"Track_ID\": track_id,\n",
    "                    \"Artist_ID\": track[\"artists\"][0][\"id\"] if track[\"artists\"] else None,\n",
    "                    \"Image_URL\": track[\"album\"][\"images\"][0][\"url\"] if track[\"album\"][\"images\"] else None,\n",
    "                    \"Duration\": track[\"duration_ms\"],\n",
    "                    \"Explicit\": track[\"explicit\"],\n",
    "                    \"Song_Release_Date\": track[\"album\"][\"release_date\"],\n",
    "                    \"Track_Number\": track[\"track_number\"],\n",
    "                    \"In Spotify API\": True\n",
    "                })\n",
    "\n",
    "            # IF THERE IS NOT DATA FROM THE SPOTIFY API, NEED TO RETRY WITH A DIFFERENT QUERY. USE GROK TO REFINE QUERY TERMS.\n",
    "            elif data is None:\n",
    "                print(f\"No track found first attempt at query: {query}\")\n",
    "                print(\"Using grok to refine Spotify Search API query\")\n",
    "                ai_song_query, ai_artist_query = grok_clean_query(song, artist)\n",
    "                print(f\"Spotify query refined via Grok: {ai_song_query} - {ai_artist_query}\")\n",
    "                query = f\"track:{ai_song_query} artist:{ai_artist_query}\"\n",
    "                params = {\"q\": query, \"type\": \"track\", \"limit\": 1}\n",
    "                data = spotify_search(query, spotify_headers, params)\n",
    "                print(\"Spotify search 2nd attempt complete\")\n",
    "\n",
    "                # NOW IF THE SPOTIFY SEARCH API WORKED, CAN UPDATE ENRICHED ROW VALUES\n",
    "                if data is not None and \"tracks\" in data and \"items\" in data[\"tracks\"] and len(data[\"tracks\"][\"items\"]) > 0:\n",
    "                    track = data[\"tracks\"][\"items\"][0]\n",
    "                    track_id = track[\"id\"]\n",
    "                    print(f\"Spotify API provided track id: {track_id}\")\n",
    "\n",
    "                    # .. CAN INCLUDE SPOTIFY API VALUES IN THE ENRICHED ROW LOOPING VARIABLE TO LATER APPEND TO THE TRACK IN SILVER..\n",
    "                    enriched_row.update({\n",
    "                        \"Track_ID\": track_id,\n",
    "                        \"Artist_ID\": track[\"artists\"][0][\"id\"] if track[\"artists\"] else None,\n",
    "                        \"Image_URL\": track[\"album\"][\"images\"][0][\"url\"] if track[\"album\"][\"images\"] else None,\n",
    "                        \"Duration\": track[\"duration_ms\"],\n",
    "                        \"Explicit\": track[\"explicit\"],\n",
    "                        \"Song_Release_Date\": track[\"album\"][\"release_date\"],\n",
    "                        \"Track_Number\": track[\"track_number\"],\n",
    "                        \"In Spotify API\": True\n",
    "                    })\n",
    "     \n",
    "            # IF THE SONG WAS FOUND IN SPOTIFY'S API, THE TRACK ID IS KNOWN ENABLING RECCOBEATS API\n",
    "            if enriched_row[\"In Spotify API\"] == True:\n",
    "                # .. MEANING WE CAN TRY TO CALL RECCOBEATS API FOR ADDITIONAL INFORMATION ON THE TRACK\n",
    "                try:\n",
    "                    print(f\"Calling Reccobeats API for: {song} - {artist}\")\n",
    "                    url = f\"https://api.reccobeats.com/v1/track?ids={track_id}\"\n",
    "                    recco_response = requests.get(url, headers={'Accept': 'application/json'}, timeout=10)\n",
    "                    recco_response.raise_for_status()\n",
    "                    recco_id = recco_response.json()[\"content\"][0][\"id\"] # RECCOBEATS ID IS DIFFERENT THAN SPOTIFY ID\n",
    "                    print(\"Reccobeats API success\")\n",
    "\n",
    "                    # IF RECCOBEATS API CALL IS SUCCESSFUL, CAN INCLUDE RECCOBEATS API VALUES IN THE ENRICHED ROW LOOPING VARIABLE TO LATER APPEND TO THE TRACK IN SILVER.\n",
    "                    feat_url = f\"https://api.reccobeats.com/v1/track/{recco_id}/audio-features\"\n",
    "                    feat = requests.get(feat_url, headers={'Accept': 'application/json'}, timeout=10).json()\n",
    "                    enriched_row.update({\n",
    "                        \"Danceability\":      feat.get(\"danceability\"),\n",
    "                        \"Energy\":            feat.get(\"energy\"),\n",
    "                        \"Key\":               feat.get(\"key\"),\n",
    "                        \"Loudness\":          feat.get(\"loudness\"),\n",
    "                        \"Mode\":              feat.get(\"mode\"),\n",
    "                        \"Speechiness\":       feat.get(\"speechiness\"),\n",
    "                        \"Acousticness\":      feat.get(\"acousticness\"),\n",
    "                        \"Instrumentalness\":  feat.get(\"instrumentalness\"),\n",
    "                        \"Liveness\":          feat.get(\"liveness\"),\n",
    "                        \"Valence\":           feat.get(\"valence\"),\n",
    "                        \"Tempo\":             feat.get(\"tempo\")\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Reccobeats failed for {track_id}: {e}\")\n",
    "                    # IF RECCOBEATS FAILED, PROCEED WITHOUT ENRICHING THE ROW\n",
    "\n",
    "            elif enriched_row[\"In Spotify API\"] == False:\n",
    "                print(f\"No tracks found for {song} - {artist}\")\n",
    "\n",
    "            # INCLUDE ROW FOR ENRICHED TRACK DATA IN CURRENT BATCH OF ENRICHED ROWS\n",
    "            enriched_rows.append(enriched_row)\n",
    "\n",
    "            # ADD TO SILVER DICT TO ALLOW FOR REUSING APPROPRIATE TRACK DATA IF TRACK IS IN A TOP 10 LIST IN A SUBSEQUENT WEEK\n",
    "            silver_dict[key] = enriched_row\n",
    "\n",
    "            time.sleep(1)  # DELAY TO PREVENT SPOTIFY API LIMIT       \n",
    "\n",
    "        print(f\"Enriched {len(enriched_rows)} rows for batch {i//BATCH_SIZE + 1}\")\n",
    "\n",
    "        # APPEND BATCH OF 10 ROWS\n",
    "        if enriched_rows:\n",
    "            spark.createDataFrame(enriched_rows, schema=silver_schema).write \\\n",
    "                .format(\"delta\").mode(\"append\").saveAsTable(silver_table)\n",
    "            print(f\"  Appended batch {i//BATCH_SIZE + 1}\")\n",
    "        else:\n",
    "            print(f\"  No rows to append for batch {i//BATCH_SIZE + 1}\")\n",
    "\n",
    "else:\n",
    "    print(\"No new songs to append.\")\n",
    "\n",
    "# PRINT FINAL COUNT OF SILVER TABLE\n",
    "final_count = spark.table(silver_table).count()\n",
    "print(f\"Silver table final row count: {final_count}\")\n",
    "\n",
    "# OPTIMIZE AND Z ORDER\n",
    "_ = spark.sql(\"OPTIMIZE {} ZORDER BY (Date, Rank)\".format(silver_table))\n",
    "print(\"Silver optimized with ZORDER by (Date, Rank). Ingestion complete.\")\n",
    "\n",
    "# TABLE TO SHOW SPOTIFY SUCCESS RATE\n",
    "print(\"Spotify enrichment status:\")\n",
    "spark.table(silver_table).groupBy(\"In Spotify API\").count().show()\n",
    "spark.table(silver_table) \\\n",
    "    .groupBy(col(\"Danceability\").isNotNull().alias(\"In Reccobeats API\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"In Reccobeats API\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02db91b1-748a-4bb9-9d18-57585b250e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- BEFORE CREATING GOLD TABLES, FIRST MUST ALTER SILVER TABLE TO ENABLE COLUMN MAPPING\n",
    "\n",
    "ALTER TABLE silver.hot100_clean \n",
    "SET TBLPROPERTIES (\n",
    "  'delta.columnMapping.mode' = 'name',\n",
    "  'delta.minReaderVersion' = '2',\n",
    "  'delta.minWriterVersion' = '5'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "147bc926-5a3b-4ab2-9d4c-4a2e4ebaa38d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CREATE GOLD PATHS IF NOT IN AWS MOUNT ALREADY.\n",
    "# dbutils.fs.mkdirs(\"/mnt/billboard_mount/gold/\")\n",
    "# dbutils.fs.mkdirs(\"/mnt/billboard_mount/gold_export/\")\n",
    "# print(\"Folders created: gold/ and gold_export/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2a3df38-84d3-46b2-8e5d-151db58b269c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-------- IF NEED TO REDO ANY TABLES IN GOLD, RUN THIS\n",
    "-- DROP TABLE IF EXISTS gold.weekly_top10_snapshot;\n",
    "-- DROP TABLE IF EXISTS gold.artist_performance;\n",
    "--DROP TABLE IF EXISTS gold.song_lifecycle;\n",
    "--DROP TABLE IF EXISTS gold.decade_summary\n",
    "--DROP TABLE IF EXISTS gold.reign_tracker\n",
    "--DROP TABLE IF EXISTS gold.annual_summary\n",
    "--DROP TABLE IF EXISTS gold.top10_streak_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b5d8e4-08ea-4ec2-8dd4-3056264b6ef7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Image_URL\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1762486599146}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER EXTERNAL TABLE #1, WEEKLY TOP 10\n",
    "-- Table is external so can export data as static CSV from AWS.\n",
    "-- This table contains the cleaned Top 10 data.\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS gold;\n",
    "USE SCHEMA gold;\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.weekly_top10_snapshot\n",
    "USING DELTA\n",
    "LOCATION '/mnt/billboard_mount/gold/weekly_top10_snapshot'\n",
    "PARTITIONED BY (year)\n",
    "TBLPROPERTIES (\n",
    "  'delta.columnMapping.mode' = 'name',\n",
    "  'delta.minReaderVersion' = '2',\n",
    "  'delta.minWriterVersion' = '5'\n",
    ")\n",
    "AS\n",
    "SELECT *, YEAR(Date) AS year\n",
    "FROM silver.hot100_clean;\n",
    "\n",
    "SELECT COUNT(*) AS row_count FROM gold.weekly_top10_snapshot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03869f2c-ab7e-4993-9f7a-84e9c8120cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER EXTERNAL TABLE #2, ARTIST PERFORMANCE\n",
    "-- This table contains Top 10 data aggregated by artist to get insight on individual artist performance throughout history.\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.artist_performance\n",
    "USING DELTA\n",
    "LOCATION '/mnt/billboard_mount/gold/artist_performance'\n",
    "AS\n",
    "WITH unique_songs AS (\n",
    "  SELECT DISTINCT\n",
    "    Artist,\n",
    "    Song,\n",
    "    Track_ID,\n",
    "    Duration,\n",
    "    Explicit,\n",
    "    Danceability,\n",
    "    Energy,\n",
    "    Tempo,\n",
    "    `Key`,\n",
    "    Mode\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Track_ID IS NOT NULL\n",
    "),\n",
    "song_counts AS (\n",
    "  SELECT\n",
    "    Artist,\n",
    "    Song,\n",
    "    COUNT(*) AS weeks_in_top10\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  GROUP BY Artist, Song\n",
    "),\n",
    "artist_song_summary AS (\n",
    "  SELECT\n",
    "    us.Artist,\n",
    "    COUNT(DISTINCT us.Song) AS total_unique_songs,\n",
    "    SUM(CASE WHEN sc.weeks_in_top10 >= 1 THEN 1 ELSE 0 END) AS total_top10_songs,\n",
    "    SUM(CASE WHEN MIN_Rank = 1 THEN 1 ELSE 0 END) AS number_ones,\n",
    "    SUM(CASE WHEN us.Explicit THEN 1 ELSE 0 END) AS explicit_songs,\n",
    "    AVG(us.Duration) AS avg_duration_ms,\n",
    "    AVG(us.Tempo) AS avg_tempo,\n",
    "    AVG(us.`Key`) AS avg_key,\n",
    "    AVG(us.Mode) AS avg_mode,\n",
    "    SUM(sc.weeks_in_top10) AS total_weeks_with_top10_entry\n",
    "  FROM unique_songs us\n",
    "  JOIN (\n",
    "    SELECT Artist, Song, MIN(Rank) AS MIN_Rank\n",
    "    FROM gold.weekly_top10_snapshot\n",
    "    GROUP BY Artist, Song\n",
    "  ) peak ON us.Artist = peak.Artist AND us.Song = peak.Song\n",
    "  JOIN song_counts sc ON us.Artist = sc.Artist AND us.Song = sc.Song\n",
    "  GROUP BY us.Artist\n",
    ")\n",
    "SELECT\n",
    "  Artist,\n",
    "  total_unique_songs,\n",
    "  total_top10_songs,\n",
    "  number_ones,\n",
    "  explicit_songs,\n",
    "  avg_duration_ms,\n",
    "  avg_tempo,\n",
    "  avg_key,\n",
    "  avg_mode,\n",
    "  total_weeks_with_top10_entry\n",
    "FROM artist_song_summary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de1a38aa-7c11-448b-86ce-4a7666e757d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER EXTERNAL TABLE #3, SONG SPECIFIC DATA\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.song_lifecycle\n",
    "USING DELTA\n",
    "LOCATION '/mnt/billboard_mount/gold/song_lifecycle'\n",
    "AS\n",
    "WITH ranked AS (\n",
    "  SELECT\n",
    "    Song, Artist, Date, Rank,\n",
    "    ROW_NUMBER() OVER (PARTITION BY Song, Artist ORDER BY Date) AS week_num,\n",
    "    MIN(Rank) OVER (PARTITION BY Song, Artist) AS peak_rank\n",
    "  FROM gold.weekly_top10_snapshot\n",
    ")\n",
    "SELECT\n",
    "  Song, Artist,\n",
    "  MIN(Date) AS debut_date,\n",
    "  MAX(Date) AS last_date,\n",
    "  peak_rank,\n",
    "  COUNT(*) AS total_weeks,\n",
    "  MIN(week_num) FILTER (WHERE Rank = peak_rank) AS weeks_to_peak\n",
    "FROM ranked\n",
    "GROUP BY Song, Artist, peak_rank;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c4ff43-bf09-4d98-8c3c-b08fa471e561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER EXTERNAL TABLE #4, DECADE SUMMARY\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.decade_summary\n",
    "USING DELTA\n",
    "LOCATION '/mnt/billboard_mount/gold/decade_summary'\n",
    "AS\n",
    "SELECT\n",
    "  FLOOR(YEAR(Date)/10)*10 AS decade,\n",
    "  COUNT(*) AS total_songs,\n",
    "  \n",
    "  AVG(Duration) AS avg_duration_ms,\n",
    "  AVG(Danceability) AS avg_danceability,\n",
    "  AVG(Energy) AS avg_energy,\n",
    "  AVG(`Key`) AS avg_key,\n",
    "  AVG(Loudness) AS avg_loudness,\n",
    "  AVG(Mode) AS avg_mode,\n",
    "  AVG(Speechiness) AS avg_speechiness,\n",
    "  AVG(Acousticness) AS avg_acousticness,\n",
    "  AVG(Instrumentalness) AS avg_instrumentalness,\n",
    "  AVG(Liveness) AS avg_liveness,\n",
    "  AVG(Valence) AS avg_valence,\n",
    "  AVG(Tempo) AS avg_tempo,\n",
    "  AVG(CASE WHEN Explicit THEN 1.0 ELSE 0.0 END) AS pct_explicit\n",
    "\n",
    "FROM (\n",
    "  -- ENSURE 1 ROW PER UNIQUE SONG TO AVOID PERSISTENT TOP 10 TRACKS FROM SKEWING DATA\n",
    "  SELECT DISTINCT\n",
    "    Song, Artist, Track_ID,\n",
    "    Duration, Explicit, Danceability, Energy, `Key`, Loudness, Mode,\n",
    "    Speechiness, Acousticness, Instrumentalness, Liveness, Valence, Tempo,\n",
    "    Date\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Track_ID IS NOT NULL\n",
    ") unique_songs\n",
    "GROUP BY FLOOR(YEAR(Date)/10)*10\n",
    "ORDER BY decade;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d127ee43-e3f5-4dee-919b-9ea67c235763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER EXTERNAL TABLE #5, ANNUAL SUMMARY\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.annual_summary\n",
    "USING DELTA\n",
    "LOCATION '/mnt/billboard_mount/gold/annual_summary'\n",
    "PARTITIONED BY (year)\n",
    "AS\n",
    "SELECT\n",
    "  YEAR(Date) AS year,\n",
    "  COUNT(*) AS total_songs,\n",
    "\n",
    "  -- Audio & Metadata (averaged over unique songs only)\n",
    "  AVG(Duration) AS avg_duration_ms,\n",
    "  AVG(Danceability) AS avg_danceability,\n",
    "  AVG(Energy) AS avg_energy,\n",
    "  AVG(`Key`) AS avg_key,\n",
    "  AVG(Loudness) AS avg_loudness,\n",
    "  AVG(Mode) AS avg_mode,\n",
    "  AVG(Speechiness) AS avg_speechiness,\n",
    "  AVG(Acousticness) AS avg_acousticness,\n",
    "  AVG(Instrumentalness) AS avg_instrumentalness,\n",
    "  AVG(Liveness) AS avg_liveness,\n",
    "  AVG(Valence) AS avg_valence,\n",
    "  AVG(Tempo) AS avg_tempo,\n",
    "\n",
    "  -- % of songs that are explicit\n",
    "  AVG(CASE WHEN Explicit THEN 1.0 ELSE 0.0 END) AS pct_explicit\n",
    "\n",
    "FROM (\n",
    "  -- One row per unique song that appeared in Top 10 that year\n",
    "  SELECT DISTINCT\n",
    "    Song, Artist, Track_ID,\n",
    "    Duration, Explicit, Danceability, Energy, `Key`, Loudness, Mode,\n",
    "    Speechiness, Acousticness, Instrumentalness, Liveness, Valence, Tempo,\n",
    "    Date\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Track_ID IS NOT NULL\n",
    ") unique_songs\n",
    "GROUP BY YEAR(Date)\n",
    "ORDER BY year;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5fbcd7-3db6-48ad-b57b-584984561c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER EXTERNAL TABLE #6, SONGS AT #1 FOR CONSECUTIVE WEEKS\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.reign_tracker\n",
    "USING DELTA\n",
    "LOCATION '/mnt/billboard_mount/gold/reign_tracker'\n",
    "AS\n",
    "WITH ranked AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    LAG(Date) OVER (PARTITION BY Song, Artist ORDER BY Date) AS prev_date\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Rank = 1\n",
    "),\n",
    "streaks AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    CASE \n",
    "      WHEN prev_date IS NULL OR DATEDIFF(Date, prev_date) > 7 \n",
    "      THEN 1 \n",
    "      ELSE 0 \n",
    "    END AS new_streak\n",
    "  FROM ranked\n",
    "),\n",
    "streak_groups AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    SUM(new_streak) OVER (PARTITION BY Song, Artist ORDER BY Date) AS streak_id\n",
    "  FROM streaks\n",
    ")\n",
    "SELECT\n",
    "  Song,\n",
    "  Artist,\n",
    "  MIN(Date) AS start_date,\n",
    "  MAX(Date) AS end_date,\n",
    "  COUNT(*) AS streak_weeks\n",
    "FROM streak_groups\n",
    "GROUP BY Song, Artist, streak_id\n",
    "HAVING COUNT(*) >= 1\n",
    "ORDER BY streak_weeks DESC;\n",
    "\n",
    "SELECT * FROM gold.reign_tracker;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a809b5bd-b96e-4184-814a-174aa00511d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GOLD LAYER EXTERNAL TABLE #7, SONGS IN TOP 10 FOR CONSECUTIVE WEEKS\n",
    "\n",
    "CREATE OR REPLACE TABLE gold.top10_streak_tracker\n",
    "USING DELTA\n",
    "LOCATION '/mnt/billboard_mount/gold/top10_streak_tracker'\n",
    "AS\n",
    "WITH ranked AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    ROW_NUMBER() OVER (PARTITION BY Song, Artist ORDER BY Date) AS rn,\n",
    "    LAG(Date) OVER (PARTITION BY Song, Artist ORDER BY Date) AS prev_date\n",
    "  FROM gold.weekly_top10_snapshot\n",
    "  WHERE Rank <= 10\n",
    "),\n",
    "streaks AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    rn,\n",
    "    CASE \n",
    "      WHEN prev_date IS NULL THEN 1 -- means it was not in the previous weekly Top 10, so not on a streak\n",
    "      WHEN DATEDIFF(Date, prev_date) = 7 THEN 0  -- if DATEDIFF = 7, that means consecutive weeks so continue streak\n",
    "      ELSE 1  -- if gap in top 10 appearances, reset to 1 as it is no longer consecutive\n",
    "    END AS is_new_streak\n",
    "  FROM ranked\n",
    "),\n",
    "streak_groups AS (\n",
    "  SELECT\n",
    "    Song,\n",
    "    Artist,\n",
    "    Date,\n",
    "    SUM(is_new_streak) OVER (PARTITION BY Song, Artist ORDER BY Date) AS streak_id\n",
    "  FROM streaks\n",
    ")\n",
    "SELECT\n",
    "  Song,\n",
    "  Artist,\n",
    "  MIN(Date) AS start_date,\n",
    "  MAX(Date) AS end_date,\n",
    "  COUNT(*) AS streak_weeks\n",
    "FROM streak_groups\n",
    "GROUP BY Song, Artist, streak_id\n",
    "HAVING COUNT(*) >= 1\n",
    "ORDER BY streak_weeks DESC, start_date;\n",
    "\n",
    "SELECT * FROM gold.top10_streak_tracker LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d330872a-bf9d-455c-bb49-05d4c8dbeba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EXPORT ALL GOLD TABLES AS CSV FILES, 1 FILE PER TABLE\n",
    "\n",
    "gold_export_path = \"/mnt/billboard_mount/gold_export/\"\n",
    "\n",
    "# 1. Clear\n",
    "dbutils.fs.rm(gold_export_path, recurse=True)\n",
    "dbutils.fs.mkdirs(gold_export_path)\n",
    "print(\"Cleared gold_export/\")\n",
    "\n",
    "# 2. Get REAL gold tables only\n",
    "gold_tables = [row.tableName for row in spark.sql(\"SHOW TABLES IN gold\").collect() \n",
    "               if row.database == \"gold\" and not row.tableName.startswith(\"_\")]\n",
    "\n",
    "print(f\"Exporting {len(gold_tables)} tables: {gold_tables}\")\n",
    "\n",
    "# 3. Export + rename\n",
    "for table in gold_tables:\n",
    "    export_dir = gold_export_path+table\n",
    "    \n",
    "    spark.table(f\"gold.{table}\").repartition(1) \\\n",
    "            .write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(export_dir)\n",
    "    \n",
    "    # Rename part-*  table.csv\n",
    "    csv_files = [f for f in dbutils.fs.ls(export_dir) if f.name.startswith(\"part-\") and f.name.endswith(\".csv\")]\n",
    "    if csv_files:\n",
    "        dbutils.fs.mv(csv_files[0].path, f\"{export_dir}/{table}.csv\")\n",
    "        print(f\"Exported: {table}.csv\")\n",
    "    else:\n",
    "        print(f\"Warning: No CSV in {table}\")\n",
    "\n",
    "print(\"All done! CSVs ready for Tableau.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 9005887144713656,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "billboard-top-100 Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
